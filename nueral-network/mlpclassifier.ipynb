{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceaa059e",
   "metadata": {},
   "source": [
    "conda install -c conda-forge scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0fff274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "input_data = iris.data\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffd1b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d67c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data = iris.target\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d80b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7663ebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c4b07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bf603",
   "metadata": {},
   "source": [
    "# Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ece85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4), (120,), (30,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.2)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636dbbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.09856764\n",
      "Iteration 2, loss = 1.09755134\n",
      "Iteration 3, loss = 1.09688584\n",
      "Iteration 4, loss = 1.09616077\n",
      "Iteration 5, loss = 1.09537176\n",
      "Iteration 6, loss = 1.09473704\n",
      "Iteration 7, loss = 1.09442473\n",
      "Iteration 8, loss = 1.09394036\n",
      "Iteration 9, loss = 1.09320951\n",
      "Iteration 10, loss = 1.09264115\n",
      "Iteration 11, loss = 1.09233879\n",
      "Iteration 12, loss = 1.09193355\n",
      "Iteration 13, loss = 1.09171501\n",
      "Iteration 14, loss = 1.09133844\n",
      "Iteration 15, loss = 1.09090581\n",
      "Iteration 16, loss = 1.09074350\n",
      "Iteration 17, loss = 1.09036149\n",
      "Iteration 18, loss = 1.09002509\n",
      "Iteration 19, loss = 1.08977056\n",
      "Iteration 20, loss = 1.08952823\n",
      "Iteration 21, loss = 1.08939784\n",
      "Iteration 22, loss = 1.08902634\n",
      "Iteration 23, loss = 1.08880825\n",
      "Iteration 24, loss = 1.08847761\n",
      "Iteration 25, loss = 1.08823098\n",
      "Iteration 26, loss = 1.08793747\n",
      "Iteration 27, loss = 1.08768661\n",
      "Iteration 28, loss = 1.08742296\n",
      "Iteration 29, loss = 1.08708974\n",
      "Iteration 30, loss = 1.08675721\n",
      "Iteration 31, loss = 1.08640518\n",
      "Iteration 32, loss = 1.08615024\n",
      "Iteration 33, loss = 1.08574722\n",
      "Iteration 34, loss = 1.08538070\n",
      "Iteration 35, loss = 1.08503967\n",
      "Iteration 36, loss = 1.08458343\n",
      "Iteration 37, loss = 1.08416070\n",
      "Iteration 38, loss = 1.08375149\n",
      "Iteration 39, loss = 1.08332198\n",
      "Iteration 40, loss = 1.08295635\n",
      "Iteration 41, loss = 1.08239413\n",
      "Iteration 42, loss = 1.08207371\n",
      "Iteration 43, loss = 1.08155156\n",
      "Iteration 44, loss = 1.08091023\n",
      "Iteration 45, loss = 1.08040141\n",
      "Iteration 46, loss = 1.07986486\n",
      "Iteration 47, loss = 1.07931263\n",
      "Iteration 48, loss = 1.07875952\n",
      "Iteration 49, loss = 1.07816263\n",
      "Iteration 50, loss = 1.07750272\n",
      "Iteration 51, loss = 1.07710148\n",
      "Iteration 52, loss = 1.07623583\n",
      "Iteration 53, loss = 1.07561594\n",
      "Iteration 54, loss = 1.07488886\n",
      "Iteration 55, loss = 1.07422587\n",
      "Iteration 56, loss = 1.07353209\n",
      "Iteration 57, loss = 1.07279572\n",
      "Iteration 58, loss = 1.07206328\n",
      "Iteration 59, loss = 1.07126956\n",
      "Iteration 60, loss = 1.07041604\n",
      "Iteration 61, loss = 1.06962596\n",
      "Iteration 62, loss = 1.06875589\n",
      "Iteration 63, loss = 1.06792414\n",
      "Iteration 64, loss = 1.06710064\n",
      "Iteration 65, loss = 1.06612973\n",
      "Iteration 66, loss = 1.06522728\n",
      "Iteration 67, loss = 1.06425374\n",
      "Iteration 68, loss = 1.06334111\n",
      "Iteration 69, loss = 1.06238916\n",
      "Iteration 70, loss = 1.06148531\n",
      "Iteration 71, loss = 1.06032307\n",
      "Iteration 72, loss = 1.05921921\n",
      "Iteration 73, loss = 1.05815009\n",
      "Iteration 74, loss = 1.05717584\n",
      "Iteration 75, loss = 1.05606945\n",
      "Iteration 76, loss = 1.05482625\n",
      "Iteration 77, loss = 1.05365976\n",
      "Iteration 78, loss = 1.05246919\n",
      "Iteration 79, loss = 1.05134155\n",
      "Iteration 80, loss = 1.05006559\n",
      "Iteration 81, loss = 1.04881626\n",
      "Iteration 82, loss = 1.04764516\n",
      "Iteration 83, loss = 1.04621129\n",
      "Iteration 84, loss = 1.04503920\n",
      "Iteration 85, loss = 1.04351477\n",
      "Iteration 86, loss = 1.04221326\n",
      "Iteration 87, loss = 1.04078607\n",
      "Iteration 88, loss = 1.03933747\n",
      "Iteration 89, loss = 1.03792695\n",
      "Iteration 90, loss = 1.03637567\n",
      "Iteration 91, loss = 1.03488013\n",
      "Iteration 92, loss = 1.03333005\n",
      "Iteration 93, loss = 1.03180160\n",
      "Iteration 94, loss = 1.03021313\n",
      "Iteration 95, loss = 1.02852276\n",
      "Iteration 96, loss = 1.02683076\n",
      "Iteration 97, loss = 1.02535670\n",
      "Iteration 98, loss = 1.02339262\n",
      "Iteration 99, loss = 1.02165576\n",
      "Iteration 100, loss = 1.01990171\n",
      "Iteration 101, loss = 1.01805886\n",
      "Iteration 102, loss = 1.01623808\n",
      "Iteration 103, loss = 1.01437875\n",
      "Iteration 104, loss = 1.01236033\n",
      "Iteration 105, loss = 1.01047981\n",
      "Iteration 106, loss = 1.00837891\n",
      "Iteration 107, loss = 1.00639399\n",
      "Iteration 108, loss = 1.00435415\n",
      "Iteration 109, loss = 1.00220854\n",
      "Iteration 110, loss = 1.00002530\n",
      "Iteration 111, loss = 0.99795193\n",
      "Iteration 112, loss = 0.99562095\n",
      "Iteration 113, loss = 0.99337868\n",
      "Iteration 114, loss = 0.99108085\n",
      "Iteration 115, loss = 0.98871393\n",
      "Iteration 116, loss = 0.98626756\n",
      "Iteration 117, loss = 0.98386232\n",
      "Iteration 118, loss = 0.98130572\n",
      "Iteration 119, loss = 0.97880761\n",
      "Iteration 120, loss = 0.97629451\n",
      "Iteration 121, loss = 0.97355791\n",
      "Iteration 122, loss = 0.97099073\n",
      "Iteration 123, loss = 0.96823011\n",
      "Iteration 124, loss = 0.96545763\n",
      "Iteration 125, loss = 0.96265972\n",
      "Iteration 126, loss = 0.95981385\n",
      "Iteration 127, loss = 0.95695907\n",
      "Iteration 128, loss = 0.95395855\n",
      "Iteration 129, loss = 0.95105256\n",
      "Iteration 130, loss = 0.94802966\n",
      "Iteration 131, loss = 0.94511455\n",
      "Iteration 132, loss = 0.94193405\n",
      "Iteration 133, loss = 0.93898328\n",
      "Iteration 134, loss = 0.93563819\n",
      "Iteration 135, loss = 0.93257134\n",
      "Iteration 136, loss = 0.92928895\n",
      "Iteration 137, loss = 0.92611913\n",
      "Iteration 138, loss = 0.92290810\n",
      "Iteration 139, loss = 0.91949196\n",
      "Iteration 140, loss = 0.91609507\n",
      "Iteration 141, loss = 0.91271230\n",
      "Iteration 142, loss = 0.90930675\n",
      "Iteration 143, loss = 0.90598142\n",
      "Iteration 144, loss = 0.90255899\n",
      "Iteration 145, loss = 0.89900251\n",
      "Iteration 146, loss = 0.89544130\n",
      "Iteration 147, loss = 0.89191562\n",
      "Iteration 148, loss = 0.88845776\n",
      "Iteration 149, loss = 0.88512155\n",
      "Iteration 150, loss = 0.88126127\n",
      "Iteration 151, loss = 0.87771962\n",
      "Iteration 152, loss = 0.87406996\n",
      "Iteration 153, loss = 0.87072664\n",
      "Iteration 154, loss = 0.86687644\n",
      "Iteration 155, loss = 0.86326467\n",
      "Iteration 156, loss = 0.85965745\n",
      "Iteration 157, loss = 0.85595824\n",
      "Iteration 158, loss = 0.85235488\n",
      "Iteration 159, loss = 0.84875496\n",
      "Iteration 160, loss = 0.84501691\n",
      "Iteration 161, loss = 0.84133569\n",
      "Iteration 162, loss = 0.83769642\n",
      "Iteration 163, loss = 0.83416917\n",
      "Iteration 164, loss = 0.83039953\n",
      "Iteration 165, loss = 0.82682887\n",
      "Iteration 166, loss = 0.82305375\n",
      "Iteration 167, loss = 0.81944939\n",
      "Iteration 168, loss = 0.81584171\n",
      "Iteration 169, loss = 0.81223161\n",
      "Iteration 170, loss = 0.80866073\n",
      "Iteration 171, loss = 0.80498401\n",
      "Iteration 172, loss = 0.80153750\n",
      "Iteration 173, loss = 0.79804077\n",
      "Iteration 174, loss = 0.79432696\n",
      "Iteration 175, loss = 0.79091261\n",
      "Iteration 176, loss = 0.78734168\n",
      "Iteration 177, loss = 0.78390079\n",
      "Iteration 178, loss = 0.78041994\n",
      "Iteration 179, loss = 0.77702499\n",
      "Iteration 180, loss = 0.77363009\n",
      "Iteration 181, loss = 0.77021821\n",
      "Iteration 182, loss = 0.76684006\n",
      "Iteration 183, loss = 0.76355977\n",
      "Iteration 184, loss = 0.76017880\n",
      "Iteration 185, loss = 0.75695004\n",
      "Iteration 186, loss = 0.75367405\n",
      "Iteration 187, loss = 0.75046543\n",
      "Iteration 188, loss = 0.74720784\n",
      "Iteration 189, loss = 0.74409489\n",
      "Iteration 190, loss = 0.74101118\n",
      "Iteration 191, loss = 0.73776777\n",
      "Iteration 192, loss = 0.73466154\n",
      "Iteration 193, loss = 0.73162801\n",
      "Iteration 194, loss = 0.72868506\n",
      "Iteration 195, loss = 0.72563772\n",
      "Iteration 196, loss = 0.72253077\n",
      "Iteration 197, loss = 0.71963739\n",
      "Iteration 198, loss = 0.71668890\n",
      "Iteration 199, loss = 0.71378269\n",
      "Iteration 200, loss = 0.71091761\n",
      "Iteration 201, loss = 0.70808694\n",
      "Iteration 202, loss = 0.70527447\n",
      "Iteration 203, loss = 0.70244496\n",
      "Iteration 204, loss = 0.69967874\n",
      "Iteration 205, loss = 0.69694947\n",
      "Iteration 206, loss = 0.69423702\n",
      "Iteration 207, loss = 0.69160005\n",
      "Iteration 208, loss = 0.68898148\n",
      "Iteration 209, loss = 0.68625971\n",
      "Iteration 210, loss = 0.68368469\n",
      "Iteration 211, loss = 0.68111099\n",
      "Iteration 212, loss = 0.67849987\n",
      "Iteration 213, loss = 0.67600029\n",
      "Iteration 214, loss = 0.67353289\n",
      "Iteration 215, loss = 0.67107189\n",
      "Iteration 216, loss = 0.66861113\n",
      "Iteration 217, loss = 0.66622683\n",
      "Iteration 218, loss = 0.66380923\n",
      "Iteration 219, loss = 0.66140079\n",
      "Iteration 220, loss = 0.65902922\n",
      "Iteration 221, loss = 0.65668359\n",
      "Iteration 222, loss = 0.65436395\n",
      "Iteration 223, loss = 0.65215635\n",
      "Iteration 224, loss = 0.64983997\n",
      "Iteration 225, loss = 0.64759772\n",
      "Iteration 226, loss = 0.64535885\n",
      "Iteration 227, loss = 0.64324033\n",
      "Iteration 228, loss = 0.64110399\n",
      "Iteration 229, loss = 0.63902753\n",
      "Iteration 230, loss = 0.63675890\n",
      "Iteration 231, loss = 0.63469844\n",
      "Iteration 232, loss = 0.63258621\n",
      "Iteration 233, loss = 0.63054299\n",
      "Iteration 234, loss = 0.62856456\n",
      "Iteration 235, loss = 0.62643114\n",
      "Iteration 236, loss = 0.62452509\n",
      "Iteration 237, loss = 0.62241042\n",
      "Iteration 238, loss = 0.62053954\n",
      "Iteration 239, loss = 0.61854589\n",
      "Iteration 240, loss = 0.61666575\n",
      "Iteration 241, loss = 0.61477009\n",
      "Iteration 242, loss = 0.61289566\n",
      "Iteration 243, loss = 0.61099942\n",
      "Iteration 244, loss = 0.60918322\n",
      "Iteration 245, loss = 0.60745294\n",
      "Iteration 246, loss = 0.60548062\n",
      "Iteration 247, loss = 0.60373646\n",
      "Iteration 248, loss = 0.60187938\n",
      "Iteration 249, loss = 0.60012706\n",
      "Iteration 250, loss = 0.59836254\n",
      "Iteration 251, loss = 0.59661247\n",
      "Iteration 252, loss = 0.59491101\n",
      "Iteration 253, loss = 0.59317646\n",
      "Iteration 254, loss = 0.59148923\n",
      "Iteration 255, loss = 0.58977951\n",
      "Iteration 256, loss = 0.58815328\n",
      "Iteration 257, loss = 0.58647397\n",
      "Iteration 258, loss = 0.58487113\n",
      "Iteration 259, loss = 0.58331204\n",
      "Iteration 260, loss = 0.58178020\n",
      "Iteration 261, loss = 0.57999391\n",
      "Iteration 262, loss = 0.57842965\n",
      "Iteration 263, loss = 0.57684197\n",
      "Iteration 264, loss = 0.57528991\n",
      "Iteration 265, loss = 0.57371184\n",
      "Iteration 266, loss = 0.57220073\n",
      "Iteration 267, loss = 0.57062950\n",
      "Iteration 268, loss = 0.56919631\n",
      "Iteration 269, loss = 0.56761885\n",
      "Iteration 270, loss = 0.56618857\n",
      "Iteration 271, loss = 0.56466694\n",
      "Iteration 272, loss = 0.56318427\n",
      "Iteration 273, loss = 0.56176132\n",
      "Iteration 274, loss = 0.56028133\n",
      "Iteration 275, loss = 0.55882169\n",
      "Iteration 276, loss = 0.55751825\n",
      "Iteration 277, loss = 0.55606487\n",
      "Iteration 278, loss = 0.55460575\n",
      "Iteration 279, loss = 0.55314682\n",
      "Iteration 280, loss = 0.55177400\n",
      "Iteration 281, loss = 0.55036331\n",
      "Iteration 282, loss = 0.54899340\n",
      "Iteration 283, loss = 0.54763612\n",
      "Iteration 284, loss = 0.54628115\n",
      "Iteration 285, loss = 0.54492594\n",
      "Iteration 286, loss = 0.54357501\n",
      "Iteration 287, loss = 0.54220580\n",
      "Iteration 288, loss = 0.54089369\n",
      "Iteration 289, loss = 0.53966610\n",
      "Iteration 290, loss = 0.53831078\n",
      "Iteration 291, loss = 0.53691384\n",
      "Iteration 292, loss = 0.53583595\n",
      "Iteration 293, loss = 0.53437716\n",
      "Iteration 294, loss = 0.53306332\n",
      "Iteration 295, loss = 0.53180755\n",
      "Iteration 296, loss = 0.53054099\n",
      "Iteration 297, loss = 0.52928481\n",
      "Iteration 298, loss = 0.52807556\n",
      "Iteration 299, loss = 0.52669374\n",
      "Iteration 300, loss = 0.52545100\n",
      "Iteration 301, loss = 0.52432078\n",
      "Iteration 302, loss = 0.52301924\n",
      "Iteration 303, loss = 0.52178787\n",
      "Iteration 304, loss = 0.52052196\n",
      "Iteration 305, loss = 0.51931596\n",
      "Iteration 306, loss = 0.51829367\n",
      "Iteration 307, loss = 0.51694268\n",
      "Iteration 308, loss = 0.51568944\n",
      "Iteration 309, loss = 0.51467759\n",
      "Iteration 310, loss = 0.51331062\n",
      "Iteration 311, loss = 0.51209859\n",
      "Iteration 312, loss = 0.51095307\n",
      "Iteration 313, loss = 0.50979278\n",
      "Iteration 314, loss = 0.50862316\n",
      "Iteration 315, loss = 0.50739248\n",
      "Iteration 316, loss = 0.50624656\n",
      "Iteration 317, loss = 0.50508993\n",
      "Iteration 318, loss = 0.50401327\n",
      "Iteration 319, loss = 0.50280834\n",
      "Iteration 320, loss = 0.50174026\n",
      "Iteration 321, loss = 0.50057730\n",
      "Iteration 322, loss = 0.49935670\n",
      "Iteration 323, loss = 0.49822509\n",
      "Iteration 324, loss = 0.49718187\n",
      "Iteration 325, loss = 0.49612732\n",
      "Iteration 326, loss = 0.49516670\n",
      "Iteration 327, loss = 0.49379222\n",
      "Iteration 328, loss = 0.49257905\n",
      "Iteration 329, loss = 0.49151618\n",
      "Iteration 330, loss = 0.49038624\n",
      "Iteration 331, loss = 0.48933146\n",
      "Iteration 332, loss = 0.48822359\n",
      "Iteration 333, loss = 0.48705879\n",
      "Iteration 334, loss = 0.48598632\n",
      "Iteration 335, loss = 0.48516452\n",
      "Iteration 336, loss = 0.48383594\n",
      "Iteration 337, loss = 0.48278399\n",
      "Iteration 338, loss = 0.48167425\n",
      "Iteration 339, loss = 0.48060430\n",
      "Iteration 340, loss = 0.47952502\n",
      "Iteration 341, loss = 0.47854121\n",
      "Iteration 342, loss = 0.47736493\n",
      "Iteration 343, loss = 0.47644000\n",
      "Iteration 344, loss = 0.47531884\n",
      "Iteration 345, loss = 0.47421704\n",
      "Iteration 346, loss = 0.47331282\n",
      "Iteration 347, loss = 0.47224137\n",
      "Iteration 348, loss = 0.47125891\n",
      "Iteration 349, loss = 0.47030769\n",
      "Iteration 350, loss = 0.46901149\n",
      "Iteration 351, loss = 0.46793972\n",
      "Iteration 352, loss = 0.46685780\n",
      "Iteration 353, loss = 0.46586367\n",
      "Iteration 354, loss = 0.46489538\n",
      "Iteration 355, loss = 0.46381252\n",
      "Iteration 356, loss = 0.46273361\n",
      "Iteration 357, loss = 0.46169911\n",
      "Iteration 358, loss = 0.46064766\n",
      "Iteration 359, loss = 0.45966663\n",
      "Iteration 360, loss = 0.45875378\n",
      "Iteration 361, loss = 0.45762890\n",
      "Iteration 362, loss = 0.45657753\n",
      "Iteration 363, loss = 0.45570973\n",
      "Iteration 364, loss = 0.45459488\n",
      "Iteration 365, loss = 0.45380290\n",
      "Iteration 366, loss = 0.45249938\n",
      "Iteration 367, loss = 0.45162427\n",
      "Iteration 368, loss = 0.45049051\n",
      "Iteration 369, loss = 0.44954935\n",
      "Iteration 370, loss = 0.44844554\n",
      "Iteration 371, loss = 0.44759858\n",
      "Iteration 372, loss = 0.44652817\n",
      "Iteration 373, loss = 0.44551858\n",
      "Iteration 374, loss = 0.44465929\n",
      "Iteration 375, loss = 0.44357611\n",
      "Iteration 376, loss = 0.44266024\n",
      "Iteration 377, loss = 0.44167056\n",
      "Iteration 378, loss = 0.44060551\n",
      "Iteration 379, loss = 0.43978806\n",
      "Iteration 380, loss = 0.43862275\n",
      "Iteration 381, loss = 0.43762125\n",
      "Iteration 382, loss = 0.43664968\n",
      "Iteration 383, loss = 0.43590740\n",
      "Iteration 384, loss = 0.43467515\n",
      "Iteration 385, loss = 0.43371360\n",
      "Iteration 386, loss = 0.43275550\n",
      "Iteration 387, loss = 0.43181246\n",
      "Iteration 388, loss = 0.43083288\n",
      "Iteration 389, loss = 0.42990460\n",
      "Iteration 390, loss = 0.42888027\n",
      "Iteration 391, loss = 0.42796553\n",
      "Iteration 392, loss = 0.42711952\n",
      "Iteration 393, loss = 0.42600144\n",
      "Iteration 394, loss = 0.42506095\n",
      "Iteration 395, loss = 0.42414177\n",
      "Iteration 396, loss = 0.42315522\n",
      "Iteration 397, loss = 0.42222058\n",
      "Iteration 398, loss = 0.42149632\n",
      "Iteration 399, loss = 0.42046671\n",
      "Iteration 400, loss = 0.41947812\n",
      "Iteration 401, loss = 0.41849221\n",
      "Iteration 402, loss = 0.41755846\n",
      "Iteration 403, loss = 0.41667540\n",
      "Iteration 404, loss = 0.41581410\n",
      "Iteration 405, loss = 0.41479425\n",
      "Iteration 406, loss = 0.41390018\n",
      "Iteration 407, loss = 0.41280322\n",
      "Iteration 408, loss = 0.41222468\n",
      "Iteration 409, loss = 0.41110551\n",
      "Iteration 410, loss = 0.41013205\n",
      "Iteration 411, loss = 0.40915379\n",
      "Iteration 412, loss = 0.40821486\n",
      "Iteration 413, loss = 0.40732675\n",
      "Iteration 414, loss = 0.40643930\n",
      "Iteration 415, loss = 0.40551688\n",
      "Iteration 416, loss = 0.40456551\n",
      "Iteration 417, loss = 0.40368731\n",
      "Iteration 418, loss = 0.40275590\n",
      "Iteration 419, loss = 0.40198382\n",
      "Iteration 420, loss = 0.40111361\n",
      "Iteration 421, loss = 0.40010222\n",
      "Iteration 422, loss = 0.39928034\n",
      "Iteration 423, loss = 0.39837292\n",
      "Iteration 424, loss = 0.39731142\n",
      "Iteration 425, loss = 0.39674201\n",
      "Iteration 426, loss = 0.39554406\n",
      "Iteration 427, loss = 0.39511866\n",
      "Iteration 428, loss = 0.39376566\n",
      "Iteration 429, loss = 0.39285698\n",
      "Iteration 430, loss = 0.39200090\n",
      "Iteration 431, loss = 0.39110573\n",
      "Iteration 432, loss = 0.39020045\n",
      "Iteration 433, loss = 0.38939653\n",
      "Iteration 434, loss = 0.38847629\n",
      "Iteration 435, loss = 0.38758253\n",
      "Iteration 436, loss = 0.38674610\n",
      "Iteration 437, loss = 0.38580784\n",
      "Iteration 438, loss = 0.38492410\n",
      "Iteration 439, loss = 0.38421919\n",
      "Iteration 440, loss = 0.38328381\n",
      "Iteration 441, loss = 0.38235647\n",
      "Iteration 442, loss = 0.38148301\n",
      "Iteration 443, loss = 0.38059733\n",
      "Iteration 444, loss = 0.37971170\n",
      "Iteration 445, loss = 0.37884983\n",
      "Iteration 446, loss = 0.37823480\n",
      "Iteration 447, loss = 0.37725563\n",
      "Iteration 448, loss = 0.37633212\n",
      "Iteration 449, loss = 0.37552037\n",
      "Iteration 450, loss = 0.37460097\n",
      "Iteration 451, loss = 0.37379819\n",
      "Iteration 452, loss = 0.37286622\n",
      "Iteration 453, loss = 0.37220396\n",
      "Iteration 454, loss = 0.37156171\n",
      "Iteration 455, loss = 0.37050618\n",
      "Iteration 456, loss = 0.36949898\n",
      "Iteration 457, loss = 0.36870007\n",
      "Iteration 458, loss = 0.36797168\n",
      "Iteration 459, loss = 0.36705955\n",
      "Iteration 460, loss = 0.36615410\n",
      "Iteration 461, loss = 0.36535967\n",
      "Iteration 462, loss = 0.36461664\n",
      "Iteration 463, loss = 0.36371563\n",
      "Iteration 464, loss = 0.36284689\n",
      "Iteration 465, loss = 0.36208905\n",
      "Iteration 466, loss = 0.36136990\n",
      "Iteration 467, loss = 0.36037535\n",
      "Iteration 468, loss = 0.35965565\n",
      "Iteration 469, loss = 0.35879080\n",
      "Iteration 470, loss = 0.35797583\n",
      "Iteration 471, loss = 0.35714168\n",
      "Iteration 472, loss = 0.35637999\n",
      "Iteration 473, loss = 0.35541075\n",
      "Iteration 474, loss = 0.35465964\n",
      "Iteration 475, loss = 0.35384622\n",
      "Iteration 476, loss = 0.35308678\n",
      "Iteration 477, loss = 0.35216733\n",
      "Iteration 478, loss = 0.35136756\n",
      "Iteration 479, loss = 0.35059018\n",
      "Iteration 480, loss = 0.34984076\n",
      "Iteration 481, loss = 0.34918734\n",
      "Iteration 482, loss = 0.34825309\n",
      "Iteration 483, loss = 0.34745367\n",
      "Iteration 484, loss = 0.34665861\n",
      "Iteration 485, loss = 0.34579759\n",
      "Iteration 486, loss = 0.34504479\n",
      "Iteration 487, loss = 0.34422735\n",
      "Iteration 488, loss = 0.34352733\n",
      "Iteration 489, loss = 0.34281261\n",
      "Iteration 490, loss = 0.34198932\n",
      "Iteration 491, loss = 0.34122623\n",
      "Iteration 492, loss = 0.34031104\n",
      "Iteration 493, loss = 0.33973855\n",
      "Iteration 494, loss = 0.33889052\n",
      "Iteration 495, loss = 0.33798782\n",
      "Iteration 496, loss = 0.33726995\n",
      "Iteration 497, loss = 0.33642813\n",
      "Iteration 498, loss = 0.33565885\n",
      "Iteration 499, loss = 0.33494849\n",
      "Iteration 500, loss = 0.33413091\n",
      "Iteration 501, loss = 0.33344801\n",
      "Iteration 502, loss = 0.33268045\n",
      "Iteration 503, loss = 0.33185243\n",
      "Iteration 504, loss = 0.33114883\n",
      "Iteration 505, loss = 0.33033264\n",
      "Iteration 506, loss = 0.32958891\n",
      "Iteration 507, loss = 0.32885276\n",
      "Iteration 508, loss = 0.32803670\n",
      "Iteration 509, loss = 0.32743575\n",
      "Iteration 510, loss = 0.32658547\n",
      "Iteration 511, loss = 0.32578847\n",
      "Iteration 512, loss = 0.32503014\n",
      "Iteration 513, loss = 0.32429222\n",
      "Iteration 514, loss = 0.32350221\n",
      "Iteration 515, loss = 0.32277954\n",
      "Iteration 516, loss = 0.32220869\n",
      "Iteration 517, loss = 0.32167134\n",
      "Iteration 518, loss = 0.32056791\n",
      "Iteration 519, loss = 0.31989226\n",
      "Iteration 520, loss = 0.31906787\n",
      "Iteration 521, loss = 0.31837637\n",
      "Iteration 522, loss = 0.31767979\n",
      "Iteration 523, loss = 0.31700047\n",
      "Iteration 524, loss = 0.31616076\n",
      "Iteration 525, loss = 0.31542464\n",
      "Iteration 526, loss = 0.31480470\n",
      "Iteration 527, loss = 0.31405744\n",
      "Iteration 528, loss = 0.31329245\n",
      "Iteration 529, loss = 0.31261674\n",
      "Iteration 530, loss = 0.31179919\n",
      "Iteration 531, loss = 0.31120974\n",
      "Iteration 532, loss = 0.31040847\n",
      "Iteration 533, loss = 0.30962515\n",
      "Iteration 534, loss = 0.30907274\n",
      "Iteration 535, loss = 0.30835624\n",
      "Iteration 536, loss = 0.30760154\n",
      "Iteration 537, loss = 0.30683132\n",
      "Iteration 538, loss = 0.30625911\n",
      "Iteration 539, loss = 0.30547230\n",
      "Iteration 540, loss = 0.30477154\n",
      "Iteration 541, loss = 0.30407534\n",
      "Iteration 542, loss = 0.30336836\n",
      "Iteration 543, loss = 0.30260781\n",
      "Iteration 544, loss = 0.30190640\n",
      "Iteration 545, loss = 0.30124124\n",
      "Iteration 546, loss = 0.30073478\n",
      "Iteration 547, loss = 0.29983508\n",
      "Iteration 548, loss = 0.29935288\n",
      "Iteration 549, loss = 0.29853117\n",
      "Iteration 550, loss = 0.29774877\n",
      "Iteration 551, loss = 0.29713365\n",
      "Iteration 552, loss = 0.29647283\n",
      "Iteration 553, loss = 0.29577048\n",
      "Iteration 554, loss = 0.29509416\n",
      "Iteration 555, loss = 0.29438727\n",
      "Iteration 556, loss = 0.29374763\n",
      "Iteration 557, loss = 0.29301496\n",
      "Iteration 558, loss = 0.29249445\n",
      "Iteration 559, loss = 0.29189346\n",
      "Iteration 560, loss = 0.29101779\n",
      "Iteration 561, loss = 0.29037239\n",
      "Iteration 562, loss = 0.28978884\n",
      "Iteration 563, loss = 0.28918199\n",
      "Iteration 564, loss = 0.28838611\n",
      "Iteration 565, loss = 0.28770630\n",
      "Iteration 566, loss = 0.28740293\n",
      "Iteration 567, loss = 0.28644461\n",
      "Iteration 568, loss = 0.28563770\n",
      "Iteration 569, loss = 0.28517755\n",
      "Iteration 570, loss = 0.28448296\n",
      "Iteration 571, loss = 0.28378446\n",
      "Iteration 572, loss = 0.28317657\n",
      "Iteration 573, loss = 0.28246902\n",
      "Iteration 574, loss = 0.28192409\n",
      "Iteration 575, loss = 0.28133123\n",
      "Iteration 576, loss = 0.28050502\n",
      "Iteration 577, loss = 0.27992438\n",
      "Iteration 578, loss = 0.27922072\n",
      "Iteration 579, loss = 0.27870986\n",
      "Iteration 580, loss = 0.27801594\n",
      "Iteration 581, loss = 0.27736345\n",
      "Iteration 582, loss = 0.27686925\n",
      "Iteration 583, loss = 0.27611604\n",
      "Iteration 584, loss = 0.27546996\n",
      "Iteration 585, loss = 0.27487390\n",
      "Iteration 586, loss = 0.27414078\n",
      "Iteration 587, loss = 0.27359541\n",
      "Iteration 588, loss = 0.27290358\n",
      "Iteration 589, loss = 0.27237227\n",
      "Iteration 590, loss = 0.27168306\n",
      "Iteration 591, loss = 0.27101278\n",
      "Iteration 592, loss = 0.27054023\n",
      "Iteration 593, loss = 0.26983091\n",
      "Iteration 594, loss = 0.26932009\n",
      "Iteration 595, loss = 0.26869385\n",
      "Iteration 596, loss = 0.26799619\n",
      "Iteration 597, loss = 0.26735323\n",
      "Iteration 598, loss = 0.26687312\n",
      "Iteration 599, loss = 0.26652318\n",
      "Iteration 600, loss = 0.26569160\n",
      "Iteration 601, loss = 0.26502107\n",
      "Iteration 602, loss = 0.26442319\n",
      "Iteration 603, loss = 0.26387162\n",
      "Iteration 604, loss = 0.26328414\n",
      "Iteration 605, loss = 0.26262510\n",
      "Iteration 606, loss = 0.26198353\n",
      "Iteration 607, loss = 0.26140250\n",
      "Iteration 608, loss = 0.26089347\n",
      "Iteration 609, loss = 0.26022233\n",
      "Iteration 610, loss = 0.25976338\n",
      "Iteration 611, loss = 0.25903183\n",
      "Iteration 612, loss = 0.25852887\n",
      "Iteration 613, loss = 0.25793988\n",
      "Iteration 614, loss = 0.25733641\n",
      "Iteration 615, loss = 0.25681800\n",
      "Iteration 616, loss = 0.25630949\n",
      "Iteration 617, loss = 0.25561101\n",
      "Iteration 618, loss = 0.25501225\n",
      "Iteration 619, loss = 0.25453656\n",
      "Iteration 620, loss = 0.25397970\n",
      "Iteration 621, loss = 0.25334690\n",
      "Iteration 622, loss = 0.25271223\n",
      "Iteration 623, loss = 0.25220223\n",
      "Iteration 624, loss = 0.25173535\n",
      "Iteration 625, loss = 0.25128912\n",
      "Iteration 626, loss = 0.25047745\n",
      "Iteration 627, loss = 0.24998748\n",
      "Iteration 628, loss = 0.24951097\n",
      "Iteration 629, loss = 0.24897649\n",
      "Iteration 630, loss = 0.24834642\n",
      "Iteration 631, loss = 0.24784691\n",
      "Iteration 632, loss = 0.24731472\n",
      "Iteration 633, loss = 0.24661597\n",
      "Iteration 634, loss = 0.24621749\n",
      "Iteration 635, loss = 0.24550795\n",
      "Iteration 636, loss = 0.24504378\n",
      "Iteration 637, loss = 0.24442291\n",
      "Iteration 638, loss = 0.24389652\n",
      "Iteration 639, loss = 0.24333808\n",
      "Iteration 640, loss = 0.24284172\n",
      "Iteration 641, loss = 0.24228324\n",
      "Iteration 642, loss = 0.24174343\n",
      "Iteration 643, loss = 0.24118922\n",
      "Iteration 644, loss = 0.24071474\n",
      "Iteration 645, loss = 0.24033066\n",
      "Iteration 646, loss = 0.23956453\n",
      "Iteration 647, loss = 0.23905898\n",
      "Iteration 648, loss = 0.23873029\n",
      "Iteration 649, loss = 0.23819067\n",
      "Iteration 650, loss = 0.23750380\n",
      "Iteration 651, loss = 0.23698789\n",
      "Iteration 652, loss = 0.23646383\n",
      "Iteration 653, loss = 0.23597028\n",
      "Iteration 654, loss = 0.23544405\n",
      "Iteration 655, loss = 0.23495079\n",
      "Iteration 656, loss = 0.23438105\n",
      "Iteration 657, loss = 0.23386744\n",
      "Iteration 658, loss = 0.23348234\n",
      "Iteration 659, loss = 0.23284584\n",
      "Iteration 660, loss = 0.23238695\n",
      "Iteration 661, loss = 0.23189009\n",
      "Iteration 662, loss = 0.23130670\n",
      "Iteration 663, loss = 0.23081136\n",
      "Iteration 664, loss = 0.23038281\n",
      "Iteration 665, loss = 0.22982682\n",
      "Iteration 666, loss = 0.22935347\n",
      "Iteration 667, loss = 0.22881251\n",
      "Iteration 668, loss = 0.22831318\n",
      "Iteration 669, loss = 0.22781028\n",
      "Iteration 670, loss = 0.22741626\n",
      "Iteration 671, loss = 0.22682794\n",
      "Iteration 672, loss = 0.22630439\n",
      "Iteration 673, loss = 0.22586783\n",
      "Iteration 674, loss = 0.22544305\n",
      "Iteration 675, loss = 0.22488501\n",
      "Iteration 676, loss = 0.22442639\n",
      "Iteration 677, loss = 0.22392593\n",
      "Iteration 678, loss = 0.22343380\n",
      "Iteration 679, loss = 0.22290807\n",
      "Iteration 680, loss = 0.22246903\n",
      "Iteration 681, loss = 0.22199011\n",
      "Iteration 682, loss = 0.22149438\n",
      "Iteration 683, loss = 0.22106369\n",
      "Iteration 684, loss = 0.22054179\n",
      "Iteration 685, loss = 0.22007788\n",
      "Iteration 686, loss = 0.21965004\n",
      "Iteration 687, loss = 0.21908692\n",
      "Iteration 688, loss = 0.21871026\n",
      "Iteration 689, loss = 0.21824912\n",
      "Iteration 690, loss = 0.21774330\n",
      "Iteration 691, loss = 0.21734838\n",
      "Iteration 692, loss = 0.21687464\n",
      "Iteration 693, loss = 0.21629355\n",
      "Iteration 694, loss = 0.21584076\n",
      "Iteration 695, loss = 0.21584066\n",
      "Iteration 696, loss = 0.21519944\n",
      "Iteration 697, loss = 0.21453668\n",
      "Iteration 698, loss = 0.21437722\n",
      "Iteration 699, loss = 0.21364975\n",
      "Iteration 700, loss = 0.21314255\n",
      "Iteration 701, loss = 0.21276597\n",
      "Iteration 702, loss = 0.21216371\n",
      "Iteration 703, loss = 0.21172370\n",
      "Iteration 704, loss = 0.21129611\n",
      "Iteration 705, loss = 0.21095528\n",
      "Iteration 706, loss = 0.21044479\n",
      "Iteration 707, loss = 0.21003829\n",
      "Iteration 708, loss = 0.20949660\n",
      "Iteration 709, loss = 0.20916213\n",
      "Iteration 710, loss = 0.20874186\n",
      "Iteration 711, loss = 0.20818113\n",
      "Iteration 712, loss = 0.20828142\n",
      "Iteration 713, loss = 0.20727792\n",
      "Iteration 714, loss = 0.20694994\n",
      "Iteration 715, loss = 0.20672098\n",
      "Iteration 716, loss = 0.20614415\n",
      "Iteration 717, loss = 0.20574394\n",
      "Iteration 718, loss = 0.20534534\n",
      "Iteration 719, loss = 0.20498212\n",
      "Iteration 720, loss = 0.20429560\n",
      "Iteration 721, loss = 0.20397098\n",
      "Iteration 722, loss = 0.20351657\n",
      "Iteration 723, loss = 0.20305763\n",
      "Iteration 724, loss = 0.20263060\n",
      "Iteration 725, loss = 0.20223130\n",
      "Iteration 726, loss = 0.20186194\n",
      "Iteration 727, loss = 0.20137136\n",
      "Iteration 728, loss = 0.20095117\n",
      "Iteration 729, loss = 0.20060698\n",
      "Iteration 730, loss = 0.20015277\n",
      "Iteration 731, loss = 0.19978460\n",
      "Iteration 732, loss = 0.19931371\n",
      "Iteration 733, loss = 0.19884264\n",
      "Iteration 734, loss = 0.19855448\n",
      "Iteration 735, loss = 0.19826209\n",
      "Iteration 736, loss = 0.19774738\n",
      "Iteration 737, loss = 0.19732023\n",
      "Iteration 738, loss = 0.19698694\n",
      "Iteration 739, loss = 0.19653844\n",
      "Iteration 740, loss = 0.19620052\n",
      "Iteration 741, loss = 0.19568965\n",
      "Iteration 742, loss = 0.19530841\n",
      "Iteration 743, loss = 0.19489174\n",
      "Iteration 744, loss = 0.19452264\n",
      "Iteration 745, loss = 0.19415399\n",
      "Iteration 746, loss = 0.19376437\n",
      "Iteration 747, loss = 0.19330736\n",
      "Iteration 748, loss = 0.19299158\n",
      "Iteration 749, loss = 0.19252140\n",
      "Iteration 750, loss = 0.19238000\n",
      "Iteration 751, loss = 0.19175990\n",
      "Iteration 752, loss = 0.19142985\n",
      "Iteration 753, loss = 0.19097958\n",
      "Iteration 754, loss = 0.19073587\n",
      "Iteration 755, loss = 0.19046040\n",
      "Iteration 756, loss = 0.18981321\n",
      "Iteration 757, loss = 0.18949094\n",
      "Iteration 758, loss = 0.18903544\n",
      "Iteration 759, loss = 0.18874470\n",
      "Iteration 760, loss = 0.18848123\n",
      "Iteration 761, loss = 0.18822390\n",
      "Iteration 762, loss = 0.18761622\n",
      "Iteration 763, loss = 0.18721400\n",
      "Iteration 764, loss = 0.18689017\n",
      "Iteration 765, loss = 0.18649239\n",
      "Iteration 766, loss = 0.18614933\n",
      "Iteration 767, loss = 0.18602187\n",
      "Iteration 768, loss = 0.18539652\n",
      "Iteration 769, loss = 0.18504065\n",
      "Iteration 770, loss = 0.18460172\n",
      "Iteration 771, loss = 0.18429480\n",
      "Iteration 772, loss = 0.18389513\n",
      "Iteration 773, loss = 0.18355121\n",
      "Iteration 774, loss = 0.18342114\n",
      "Iteration 775, loss = 0.18283247\n",
      "Iteration 776, loss = 0.18258271\n",
      "Iteration 777, loss = 0.18215867\n",
      "Iteration 778, loss = 0.18177995\n",
      "Iteration 779, loss = 0.18187343\n",
      "Iteration 780, loss = 0.18105066\n",
      "Iteration 781, loss = 0.18073303\n",
      "Iteration 782, loss = 0.18032453\n",
      "Iteration 783, loss = 0.17997259\n",
      "Iteration 784, loss = 0.17964502\n",
      "Iteration 785, loss = 0.17926691\n",
      "Iteration 786, loss = 0.17892313\n",
      "Iteration 787, loss = 0.17866622\n",
      "Iteration 788, loss = 0.17835502\n",
      "Iteration 789, loss = 0.17810649\n",
      "Iteration 790, loss = 0.17757489\n",
      "Iteration 791, loss = 0.17734934\n",
      "Iteration 792, loss = 0.17694884\n",
      "Iteration 793, loss = 0.17676214\n",
      "Iteration 794, loss = 0.17618416\n",
      "Iteration 795, loss = 0.17586760\n",
      "Iteration 796, loss = 0.17564148\n",
      "Iteration 797, loss = 0.17534473\n",
      "Iteration 798, loss = 0.17487550\n",
      "Iteration 799, loss = 0.17454725\n",
      "Iteration 800, loss = 0.17418498\n",
      "Iteration 801, loss = 0.17388395\n",
      "Iteration 802, loss = 0.17350927\n",
      "Iteration 803, loss = 0.17319573\n",
      "Iteration 804, loss = 0.17285207\n",
      "Iteration 805, loss = 0.17262952\n",
      "Iteration 806, loss = 0.17221027\n",
      "Iteration 807, loss = 0.17189906\n",
      "Iteration 808, loss = 0.17154042\n",
      "Iteration 809, loss = 0.17122999\n",
      "Iteration 810, loss = 0.17104275\n",
      "Iteration 811, loss = 0.17082274\n",
      "Iteration 812, loss = 0.17041999\n",
      "Iteration 813, loss = 0.17007180\n",
      "Iteration 814, loss = 0.16963168\n",
      "Iteration 815, loss = 0.16930389\n",
      "Iteration 816, loss = 0.16901817\n",
      "Iteration 817, loss = 0.16867215\n",
      "Iteration 818, loss = 0.16839355\n",
      "Iteration 819, loss = 0.16808824\n",
      "Iteration 820, loss = 0.16775766\n",
      "Iteration 821, loss = 0.16746327\n",
      "Iteration 822, loss = 0.16736982\n",
      "Iteration 823, loss = 0.16678037\n",
      "Iteration 824, loss = 0.16659528\n",
      "Iteration 825, loss = 0.16624217\n",
      "Iteration 826, loss = 0.16588194\n",
      "Iteration 827, loss = 0.16576265\n",
      "Iteration 828, loss = 0.16530752\n",
      "Iteration 829, loss = 0.16495312\n",
      "Iteration 830, loss = 0.16464003\n",
      "Iteration 831, loss = 0.16448205\n",
      "Iteration 832, loss = 0.16407181\n",
      "Iteration 833, loss = 0.16379119\n",
      "Iteration 834, loss = 0.16343768\n",
      "Iteration 835, loss = 0.16340730\n",
      "Iteration 836, loss = 0.16286587\n",
      "Iteration 837, loss = 0.16258877\n",
      "Iteration 838, loss = 0.16225100\n",
      "Iteration 839, loss = 0.16198159\n",
      "Iteration 840, loss = 0.16178775\n",
      "Iteration 841, loss = 0.16143266\n",
      "Iteration 842, loss = 0.16121594\n",
      "Iteration 843, loss = 0.16081435\n",
      "Iteration 844, loss = 0.16057845\n",
      "Iteration 845, loss = 0.16023697\n",
      "Iteration 846, loss = 0.16011677\n",
      "Iteration 847, loss = 0.16001204\n",
      "Iteration 848, loss = 0.15938944\n",
      "Iteration 849, loss = 0.15907477\n",
      "Iteration 850, loss = 0.15890520\n",
      "Iteration 851, loss = 0.15869020\n",
      "Iteration 852, loss = 0.15824683\n",
      "Iteration 853, loss = 0.15803254\n",
      "Iteration 854, loss = 0.15766275\n",
      "Iteration 855, loss = 0.15732673\n",
      "Iteration 856, loss = 0.15709764\n",
      "Iteration 857, loss = 0.15686082\n",
      "Iteration 858, loss = 0.15665790\n",
      "Iteration 859, loss = 0.15628397\n",
      "Iteration 860, loss = 0.15621137\n",
      "Iteration 861, loss = 0.15571021\n",
      "Iteration 862, loss = 0.15562528\n",
      "Iteration 863, loss = 0.15525206\n",
      "Iteration 864, loss = 0.15493040\n",
      "Iteration 865, loss = 0.15461558\n",
      "Iteration 866, loss = 0.15460712\n",
      "Iteration 867, loss = 0.15441478\n",
      "Iteration 868, loss = 0.15409696\n",
      "Iteration 869, loss = 0.15365155\n",
      "Iteration 870, loss = 0.15336581\n",
      "Iteration 871, loss = 0.15312691\n",
      "Iteration 872, loss = 0.15274903\n",
      "Iteration 873, loss = 0.15288076\n",
      "Iteration 874, loss = 0.15228586\n",
      "Iteration 875, loss = 0.15221966\n",
      "Iteration 876, loss = 0.15168966\n",
      "Iteration 877, loss = 0.15141069\n",
      "Iteration 878, loss = 0.15115968\n",
      "Iteration 879, loss = 0.15094450\n",
      "Iteration 880, loss = 0.15066603\n",
      "Iteration 881, loss = 0.15039112\n",
      "Iteration 882, loss = 0.15017108\n",
      "Iteration 883, loss = 0.14998015\n",
      "Iteration 884, loss = 0.14963163\n",
      "Iteration 885, loss = 0.14956836\n",
      "Iteration 886, loss = 0.14913802\n",
      "Iteration 887, loss = 0.14887870\n",
      "Iteration 888, loss = 0.14865875\n",
      "Iteration 889, loss = 0.14843827\n",
      "Iteration 890, loss = 0.14816537\n",
      "Iteration 891, loss = 0.14794859\n",
      "Iteration 892, loss = 0.14771846\n",
      "Iteration 893, loss = 0.14745993\n",
      "Iteration 894, loss = 0.14718022\n",
      "Iteration 895, loss = 0.14686842\n",
      "Iteration 896, loss = 0.14665177\n",
      "Iteration 897, loss = 0.14641096\n",
      "Iteration 898, loss = 0.14635001\n",
      "Iteration 899, loss = 0.14591937\n",
      "Iteration 900, loss = 0.14561435\n",
      "Iteration 901, loss = 0.14536458\n",
      "Iteration 902, loss = 0.14520086\n",
      "Iteration 903, loss = 0.14492825\n",
      "Iteration 904, loss = 0.14505380\n",
      "Iteration 905, loss = 0.14440656\n",
      "Iteration 906, loss = 0.14426684\n",
      "Iteration 907, loss = 0.14401905\n",
      "Iteration 908, loss = 0.14372318\n",
      "Iteration 909, loss = 0.14352683\n",
      "Iteration 910, loss = 0.14347093\n",
      "Iteration 911, loss = 0.14301284\n",
      "Iteration 912, loss = 0.14276384\n",
      "Iteration 913, loss = 0.14261883\n",
      "Iteration 914, loss = 0.14238517\n",
      "Iteration 915, loss = 0.14209029\n",
      "Iteration 916, loss = 0.14203181\n",
      "Iteration 917, loss = 0.14174417\n",
      "Iteration 918, loss = 0.14154880\n",
      "Iteration 919, loss = 0.14114452\n",
      "Iteration 920, loss = 0.14100262\n",
      "Iteration 921, loss = 0.14073287\n",
      "Iteration 922, loss = 0.14045760\n",
      "Iteration 923, loss = 0.14046586\n",
      "Iteration 924, loss = 0.14032130\n",
      "Iteration 925, loss = 0.13981622\n",
      "Iteration 926, loss = 0.13955559\n",
      "Iteration 927, loss = 0.13933857\n",
      "Iteration 928, loss = 0.13942911\n",
      "Iteration 929, loss = 0.13895151\n",
      "Iteration 930, loss = 0.13875050\n",
      "Iteration 931, loss = 0.13841814\n",
      "Iteration 932, loss = 0.13822689\n",
      "Iteration 933, loss = 0.13809013\n",
      "Iteration 934, loss = 0.13780375\n",
      "Iteration 935, loss = 0.13778065\n",
      "Iteration 936, loss = 0.13761416\n",
      "Iteration 937, loss = 0.13712448\n",
      "Iteration 938, loss = 0.13693871\n",
      "Iteration 939, loss = 0.13672967\n",
      "Iteration 940, loss = 0.13648737\n",
      "Iteration 941, loss = 0.13633270\n",
      "Iteration 942, loss = 0.13632732\n",
      "Iteration 943, loss = 0.13631696\n",
      "Iteration 944, loss = 0.13561879\n",
      "Iteration 945, loss = 0.13549546\n",
      "Iteration 946, loss = 0.13513984\n",
      "Iteration 947, loss = 0.13496598\n",
      "Iteration 948, loss = 0.13520309\n",
      "Iteration 949, loss = 0.13469841\n",
      "Iteration 950, loss = 0.13451144\n",
      "Iteration 951, loss = 0.13418287\n",
      "Iteration 952, loss = 0.13402442\n",
      "Iteration 953, loss = 0.13370621\n",
      "Iteration 954, loss = 0.13352597\n",
      "Iteration 955, loss = 0.13330911\n",
      "Iteration 956, loss = 0.13309940\n",
      "Iteration 957, loss = 0.13293460\n",
      "Iteration 958, loss = 0.13278282\n",
      "Iteration 959, loss = 0.13250897\n",
      "Iteration 960, loss = 0.13251687\n",
      "Iteration 961, loss = 0.13219742\n",
      "Iteration 962, loss = 0.13189930\n",
      "Iteration 963, loss = 0.13170504\n",
      "Iteration 964, loss = 0.13148844\n",
      "Iteration 965, loss = 0.13140399\n",
      "Iteration 966, loss = 0.13165095\n",
      "Iteration 967, loss = 0.13107838\n",
      "Iteration 968, loss = 0.13089870\n",
      "Iteration 969, loss = 0.13071383\n",
      "Iteration 970, loss = 0.13035750\n",
      "Iteration 971, loss = 0.13031371\n",
      "Iteration 972, loss = 0.12991884\n",
      "Iteration 973, loss = 0.13014265\n",
      "Iteration 974, loss = 0.12955004\n",
      "Iteration 975, loss = 0.12932623\n",
      "Iteration 976, loss = 0.12918418\n",
      "Iteration 977, loss = 0.12900267\n",
      "Iteration 978, loss = 0.12891153\n",
      "Iteration 979, loss = 0.12873868\n",
      "Iteration 980, loss = 0.12837380\n",
      "Iteration 981, loss = 0.12822622\n",
      "Iteration 982, loss = 0.12798916\n",
      "Iteration 983, loss = 0.12780388\n",
      "Iteration 984, loss = 0.12775512\n",
      "Iteration 985, loss = 0.12765880\n",
      "Iteration 986, loss = 0.12732935\n",
      "Iteration 987, loss = 0.12707505\n",
      "Iteration 988, loss = 0.12690573\n",
      "Iteration 989, loss = 0.12677030\n",
      "Iteration 990, loss = 0.12674895\n",
      "Iteration 991, loss = 0.12628662\n",
      "Iteration 992, loss = 0.12630063\n",
      "Iteration 993, loss = 0.12594843\n",
      "Iteration 994, loss = 0.12638614\n",
      "Iteration 995, loss = 0.12558868\n",
      "Iteration 996, loss = 0.12531352\n",
      "Iteration 997, loss = 0.12532523\n",
      "Iteration 998, loss = 0.12508335\n",
      "Iteration 999, loss = 0.12497014\n",
      "Iteration 1000, loss = 0.12497126\n",
      "Iteration 1001, loss = 0.12464617\n",
      "Iteration 1002, loss = 0.12432678\n",
      "Iteration 1003, loss = 0.12414242\n",
      "Iteration 1004, loss = 0.12402050\n",
      "Iteration 1005, loss = 0.12380410\n",
      "Iteration 1006, loss = 0.12363586\n",
      "Iteration 1007, loss = 0.12349723\n",
      "Iteration 1008, loss = 0.12323125\n",
      "Iteration 1009, loss = 0.12304288\n",
      "Iteration 1010, loss = 0.12291730\n",
      "Iteration 1011, loss = 0.12273802\n",
      "Iteration 1012, loss = 0.12264378\n",
      "Iteration 1013, loss = 0.12245460\n",
      "Iteration 1014, loss = 0.12219405\n",
      "Iteration 1015, loss = 0.12205306\n",
      "Iteration 1016, loss = 0.12223808\n",
      "Iteration 1017, loss = 0.12184697\n",
      "Iteration 1018, loss = 0.12165593\n",
      "Iteration 1019, loss = 0.12142660\n",
      "Iteration 1020, loss = 0.12126797\n",
      "Iteration 1021, loss = 0.12165722\n",
      "Iteration 1022, loss = 0.12091057\n",
      "Iteration 1023, loss = 0.12096768\n",
      "Iteration 1024, loss = 0.12066765\n",
      "Iteration 1025, loss = 0.12034767\n",
      "Iteration 1026, loss = 0.12017891\n",
      "Iteration 1027, loss = 0.12002849\n",
      "Iteration 1028, loss = 0.11987453\n",
      "Iteration 1029, loss = 0.11969725\n",
      "Iteration 1030, loss = 0.11996444\n",
      "Iteration 1031, loss = 0.11936948\n",
      "Iteration 1032, loss = 0.11919918\n",
      "Iteration 1033, loss = 0.11908936\n",
      "Iteration 1034, loss = 0.11896720\n",
      "Iteration 1035, loss = 0.11886080\n",
      "Iteration 1036, loss = 0.11859170\n",
      "Iteration 1037, loss = 0.11854251\n",
      "Iteration 1038, loss = 0.11829647\n",
      "Iteration 1039, loss = 0.11812195\n",
      "Iteration 1040, loss = 0.11792221\n",
      "Iteration 1041, loss = 0.11786201\n",
      "Iteration 1042, loss = 0.11762079\n",
      "Iteration 1043, loss = 0.11761169\n",
      "Iteration 1044, loss = 0.11730944\n",
      "Iteration 1045, loss = 0.11766715\n",
      "Iteration 1046, loss = 0.11712823\n",
      "Iteration 1047, loss = 0.11693593\n",
      "Iteration 1048, loss = 0.11671171\n",
      "Iteration 1049, loss = 0.11650508\n",
      "Iteration 1050, loss = 0.11632029\n",
      "Iteration 1051, loss = 0.11615428\n",
      "Iteration 1052, loss = 0.11604538\n",
      "Iteration 1053, loss = 0.11618170\n",
      "Iteration 1054, loss = 0.11584020\n",
      "Iteration 1055, loss = 0.11562794\n",
      "Iteration 1056, loss = 0.11544198\n",
      "Iteration 1057, loss = 0.11530437\n",
      "Iteration 1058, loss = 0.11513315\n",
      "Iteration 1059, loss = 0.11502461\n",
      "Iteration 1060, loss = 0.11485400\n",
      "Iteration 1061, loss = 0.11466939\n",
      "Iteration 1062, loss = 0.11452935\n",
      "Iteration 1063, loss = 0.11437223\n",
      "Iteration 1064, loss = 0.11447604\n",
      "Iteration 1065, loss = 0.11418596\n",
      "Iteration 1066, loss = 0.11399478\n",
      "Iteration 1067, loss = 0.11420596\n",
      "Iteration 1068, loss = 0.11391590\n",
      "Iteration 1069, loss = 0.11365133\n",
      "Iteration 1070, loss = 0.11331504\n",
      "Iteration 1071, loss = 0.11328788\n",
      "Iteration 1072, loss = 0.11304487\n",
      "Iteration 1073, loss = 0.11286989\n",
      "Iteration 1074, loss = 0.11281453\n",
      "Iteration 1075, loss = 0.11262756\n",
      "Iteration 1076, loss = 0.11252425\n",
      "Iteration 1077, loss = 0.11242100\n",
      "Iteration 1078, loss = 0.11228049\n",
      "Iteration 1079, loss = 0.11203703\n",
      "Iteration 1080, loss = 0.11194778\n",
      "Iteration 1081, loss = 0.11182083\n",
      "Iteration 1082, loss = 0.11171085\n",
      "Iteration 1083, loss = 0.11149552\n",
      "Iteration 1084, loss = 0.11130518\n",
      "Iteration 1085, loss = 0.11123967\n",
      "Iteration 1086, loss = 0.11120979\n",
      "Iteration 1087, loss = 0.11095874\n",
      "Iteration 1088, loss = 0.11081071\n",
      "Iteration 1089, loss = 0.11074357\n",
      "Iteration 1090, loss = 0.11050427\n",
      "Iteration 1091, loss = 0.11047289\n",
      "Iteration 1092, loss = 0.11037218\n",
      "Iteration 1093, loss = 0.11007555\n",
      "Iteration 1094, loss = 0.11013807\n",
      "Iteration 1095, loss = 0.11006263\n",
      "Iteration 1096, loss = 0.10972973\n",
      "Iteration 1097, loss = 0.10964455\n",
      "Iteration 1098, loss = 0.10952107\n",
      "Iteration 1099, loss = 0.10941129\n",
      "Iteration 1100, loss = 0.10927552\n",
      "Iteration 1101, loss = 0.10899272\n",
      "Iteration 1102, loss = 0.10888444\n",
      "Iteration 1103, loss = 0.10873133\n",
      "Iteration 1104, loss = 0.10868601\n",
      "Iteration 1105, loss = 0.10849720\n",
      "Iteration 1106, loss = 0.10833271\n",
      "Iteration 1107, loss = 0.10821737\n",
      "Iteration 1108, loss = 0.10808159\n",
      "Iteration 1109, loss = 0.10796448\n",
      "Iteration 1110, loss = 0.10778887\n",
      "Iteration 1111, loss = 0.10781927\n",
      "Iteration 1112, loss = 0.10767458\n",
      "Iteration 1113, loss = 0.10754459\n",
      "Iteration 1114, loss = 0.10731575\n",
      "Iteration 1115, loss = 0.10729191\n",
      "Iteration 1116, loss = 0.10709320\n",
      "Iteration 1117, loss = 0.10697846\n",
      "Iteration 1118, loss = 0.10704396\n",
      "Iteration 1119, loss = 0.10669407\n",
      "Iteration 1120, loss = 0.10664960\n",
      "Iteration 1121, loss = 0.10639073\n",
      "Iteration 1122, loss = 0.10635051\n",
      "Iteration 1123, loss = 0.10616210\n",
      "Iteration 1124, loss = 0.10615455\n",
      "Iteration 1125, loss = 0.10592317\n",
      "Iteration 1126, loss = 0.10577566\n",
      "Iteration 1127, loss = 0.10569530\n",
      "Iteration 1128, loss = 0.10552103\n",
      "Iteration 1129, loss = 0.10541688\n",
      "Iteration 1130, loss = 0.10526052\n",
      "Iteration 1131, loss = 0.10513887\n",
      "Iteration 1132, loss = 0.10509310\n",
      "Iteration 1133, loss = 0.10489332\n",
      "Iteration 1134, loss = 0.10491981\n",
      "Iteration 1135, loss = 0.10488104\n",
      "Iteration 1136, loss = 0.10452652\n",
      "Iteration 1137, loss = 0.10452255\n",
      "Iteration 1138, loss = 0.10437883\n",
      "Iteration 1139, loss = 0.10428573\n",
      "Iteration 1140, loss = 0.10405050\n",
      "Iteration 1141, loss = 0.10416241\n",
      "Iteration 1142, loss = 0.10392322\n",
      "Iteration 1143, loss = 0.10369107\n",
      "Iteration 1144, loss = 0.10379157\n",
      "Iteration 1145, loss = 0.10356714\n",
      "Iteration 1146, loss = 0.10359711\n",
      "Iteration 1147, loss = 0.10337373\n",
      "Iteration 1148, loss = 0.10302966\n",
      "Iteration 1149, loss = 0.10302254\n",
      "Iteration 1150, loss = 0.10288803\n",
      "Iteration 1151, loss = 0.10292941\n",
      "Iteration 1152, loss = 0.10284637\n",
      "Iteration 1153, loss = 0.10267841\n",
      "Iteration 1154, loss = 0.10246324\n",
      "Iteration 1155, loss = 0.10230660\n",
      "Iteration 1156, loss = 0.10237239\n",
      "Iteration 1157, loss = 0.10215388\n",
      "Iteration 1158, loss = 0.10208322\n",
      "Iteration 1159, loss = 0.10208484\n",
      "Iteration 1160, loss = 0.10180625\n",
      "Iteration 1161, loss = 0.10161951\n",
      "Iteration 1162, loss = 0.10149435\n",
      "Iteration 1163, loss = 0.10173863\n",
      "Iteration 1164, loss = 0.10143450\n",
      "Iteration 1165, loss = 0.10125452\n",
      "Iteration 1166, loss = 0.10106584\n",
      "Iteration 1167, loss = 0.10096970\n",
      "Iteration 1168, loss = 0.10083281\n",
      "Iteration 1169, loss = 0.10076536\n",
      "Iteration 1170, loss = 0.10055276\n",
      "Iteration 1171, loss = 0.10070896\n",
      "Iteration 1172, loss = 0.10042308\n",
      "Iteration 1173, loss = 0.10033089\n",
      "Iteration 1174, loss = 0.10017921\n",
      "Iteration 1175, loss = 0.10010549\n",
      "Iteration 1176, loss = 0.09996176\n",
      "Iteration 1177, loss = 0.09979373\n",
      "Iteration 1178, loss = 0.09975820\n",
      "Iteration 1179, loss = 0.09965203\n",
      "Iteration 1180, loss = 0.09960005\n",
      "Iteration 1181, loss = 0.09944882\n",
      "Iteration 1182, loss = 0.09931371\n",
      "Iteration 1183, loss = 0.09924327\n",
      "Iteration 1184, loss = 0.09912250\n",
      "Iteration 1185, loss = 0.09899595\n",
      "Iteration 1186, loss = 0.09886967\n",
      "Iteration 1187, loss = 0.09874981\n",
      "Iteration 1188, loss = 0.09892757\n",
      "Iteration 1189, loss = 0.09864770\n",
      "Iteration 1190, loss = 0.09857749\n",
      "Iteration 1191, loss = 0.09835122\n",
      "Iteration 1192, loss = 0.09839078\n",
      "Iteration 1193, loss = 0.09822480\n",
      "Iteration 1194, loss = 0.09824095\n",
      "Iteration 1195, loss = 0.09795269\n",
      "Iteration 1196, loss = 0.09788133\n",
      "Iteration 1197, loss = 0.09782555\n",
      "Iteration 1198, loss = 0.09770192\n",
      "Iteration 1199, loss = 0.09754929\n",
      "Iteration 1200, loss = 0.09756428\n",
      "Iteration 1201, loss = 0.09741272\n",
      "Iteration 1202, loss = 0.09756222\n",
      "Iteration 1203, loss = 0.09725985\n",
      "Iteration 1204, loss = 0.09737514\n",
      "Iteration 1205, loss = 0.09697030\n",
      "Iteration 1206, loss = 0.09689123\n",
      "Iteration 1207, loss = 0.09670753\n",
      "Iteration 1208, loss = 0.09679626\n",
      "Iteration 1209, loss = 0.09673969\n",
      "Iteration 1210, loss = 0.09644992\n",
      "Iteration 1211, loss = 0.09638235\n",
      "Iteration 1212, loss = 0.09636798\n",
      "Iteration 1213, loss = 0.09623161\n",
      "Iteration 1214, loss = 0.09620775\n",
      "Iteration 1215, loss = 0.09599146\n",
      "Iteration 1216, loss = 0.09594970\n",
      "Iteration 1217, loss = 0.09576858\n",
      "Iteration 1218, loss = 0.09589724\n",
      "Iteration 1219, loss = 0.09571769\n",
      "Iteration 1220, loss = 0.09567575\n",
      "Iteration 1221, loss = 0.09550673\n",
      "Iteration 1222, loss = 0.09530140\n",
      "Iteration 1223, loss = 0.09517174\n",
      "Iteration 1224, loss = 0.09503895\n",
      "Iteration 1225, loss = 0.09502233\n",
      "Iteration 1226, loss = 0.09567629\n",
      "Iteration 1227, loss = 0.09487371\n",
      "Iteration 1228, loss = 0.09475074\n",
      "Iteration 1229, loss = 0.09469815\n",
      "Iteration 1230, loss = 0.09451116\n",
      "Iteration 1231, loss = 0.09499597\n",
      "Iteration 1232, loss = 0.09444105\n",
      "Iteration 1233, loss = 0.09438838\n",
      "Iteration 1234, loss = 0.09421858\n",
      "Iteration 1235, loss = 0.09412008\n",
      "Iteration 1236, loss = 0.09411735\n",
      "Iteration 1237, loss = 0.09384630\n",
      "Iteration 1238, loss = 0.09378180\n",
      "Iteration 1239, loss = 0.09367062\n",
      "Iteration 1240, loss = 0.09355091\n",
      "Iteration 1241, loss = 0.09372422\n",
      "Iteration 1242, loss = 0.09353937\n",
      "Iteration 1243, loss = 0.09354957\n",
      "Iteration 1244, loss = 0.09322623\n",
      "Iteration 1245, loss = 0.09311768\n",
      "Iteration 1246, loss = 0.09303936\n",
      "Iteration 1247, loss = 0.09298331\n",
      "Iteration 1248, loss = 0.09304945\n",
      "Iteration 1249, loss = 0.09277410\n",
      "Iteration 1250, loss = 0.09268358\n",
      "Iteration 1251, loss = 0.09274239\n",
      "Iteration 1252, loss = 0.09265596\n",
      "Iteration 1253, loss = 0.09244743\n",
      "Iteration 1254, loss = 0.09233098\n",
      "Iteration 1255, loss = 0.09235079\n",
      "Iteration 1256, loss = 0.09226434\n",
      "Iteration 1257, loss = 0.09216055\n",
      "Iteration 1258, loss = 0.09196804\n",
      "Iteration 1259, loss = 0.09186848\n",
      "Iteration 1260, loss = 0.09182576\n",
      "Iteration 1261, loss = 0.09174669\n",
      "Iteration 1262, loss = 0.09161740\n",
      "Iteration 1263, loss = 0.09168062\n",
      "Iteration 1264, loss = 0.09159275\n",
      "Iteration 1265, loss = 0.09144005\n",
      "Iteration 1266, loss = 0.09132246\n",
      "Iteration 1267, loss = 0.09123355\n",
      "Iteration 1268, loss = 0.09139022\n",
      "Iteration 1269, loss = 0.09106115\n",
      "Iteration 1270, loss = 0.09141578\n",
      "Iteration 1271, loss = 0.09105285\n",
      "Iteration 1272, loss = 0.09085552\n",
      "Iteration 1273, loss = 0.09083574\n",
      "Iteration 1274, loss = 0.09065819\n",
      "Iteration 1275, loss = 0.09057705\n",
      "Iteration 1276, loss = 0.09046687\n",
      "Iteration 1277, loss = 0.09042405\n",
      "Iteration 1278, loss = 0.09049836\n",
      "Iteration 1279, loss = 0.09021057\n",
      "Iteration 1280, loss = 0.09029976\n",
      "Iteration 1281, loss = 0.09008170\n",
      "Iteration 1282, loss = 0.09008475\n",
      "Iteration 1283, loss = 0.08999133\n",
      "Iteration 1284, loss = 0.08986739\n",
      "Iteration 1285, loss = 0.08975433\n",
      "Iteration 1286, loss = 0.08969466\n",
      "Iteration 1287, loss = 0.08976022\n",
      "Iteration 1288, loss = 0.08949930\n",
      "Iteration 1289, loss = 0.08942630\n",
      "Iteration 1290, loss = 0.08937211\n",
      "Iteration 1291, loss = 0.08929977\n",
      "Iteration 1292, loss = 0.08921912\n",
      "Iteration 1293, loss = 0.08908001\n",
      "Iteration 1294, loss = 0.08902606\n",
      "Iteration 1295, loss = 0.08905651\n",
      "Iteration 1296, loss = 0.08889499\n",
      "Iteration 1297, loss = 0.08917483\n",
      "Iteration 1298, loss = 0.08882369\n",
      "Iteration 1299, loss = 0.08854347\n",
      "Iteration 1300, loss = 0.08849911\n",
      "Iteration 1301, loss = 0.08864169\n",
      "Iteration 1302, loss = 0.08840230\n",
      "Iteration 1303, loss = 0.08830677\n",
      "Iteration 1304, loss = 0.08838882\n",
      "Iteration 1305, loss = 0.08841815\n",
      "Iteration 1306, loss = 0.08838887\n",
      "Iteration 1307, loss = 0.08832786\n",
      "Iteration 1308, loss = 0.08811654\n",
      "Iteration 1309, loss = 0.08804291\n",
      "Iteration 1310, loss = 0.08787577\n",
      "Iteration 1311, loss = 0.08771589\n",
      "Iteration 1312, loss = 0.08763864\n",
      "Iteration 1313, loss = 0.08756292\n",
      "Iteration 1314, loss = 0.08744784\n",
      "Iteration 1315, loss = 0.08753784\n",
      "Iteration 1316, loss = 0.08747182\n",
      "Iteration 1317, loss = 0.08724703\n",
      "Iteration 1318, loss = 0.08725205\n",
      "Iteration 1319, loss = 0.08735625\n",
      "Iteration 1320, loss = 0.08699815\n",
      "Iteration 1321, loss = 0.08701472\n",
      "Iteration 1322, loss = 0.08688987\n",
      "Iteration 1323, loss = 0.08679794\n",
      "Iteration 1324, loss = 0.08679531\n",
      "Iteration 1325, loss = 0.08664815\n",
      "Iteration 1326, loss = 0.08667070\n",
      "Iteration 1327, loss = 0.08648304\n",
      "Iteration 1328, loss = 0.08642229\n",
      "Iteration 1329, loss = 0.08653553\n",
      "Iteration 1330, loss = 0.08654836\n",
      "Iteration 1331, loss = 0.08626782\n",
      "Iteration 1332, loss = 0.08615871\n",
      "Iteration 1333, loss = 0.08610223\n",
      "Iteration 1334, loss = 0.08600653\n",
      "Iteration 1335, loss = 0.08606462\n",
      "Iteration 1336, loss = 0.08585281\n",
      "Iteration 1337, loss = 0.08576841\n",
      "Iteration 1338, loss = 0.08601553\n",
      "Iteration 1339, loss = 0.08562540\n",
      "Iteration 1340, loss = 0.08572864\n",
      "Iteration 1341, loss = 0.08618074\n",
      "Iteration 1342, loss = 0.08564562\n",
      "Iteration 1343, loss = 0.08532813\n",
      "Iteration 1344, loss = 0.08535781\n",
      "Iteration 1345, loss = 0.08529110\n",
      "Iteration 1346, loss = 0.08516857\n",
      "Iteration 1347, loss = 0.08525901\n",
      "Iteration 1348, loss = 0.08502704\n",
      "Iteration 1349, loss = 0.08496954\n",
      "Iteration 1350, loss = 0.08499277\n",
      "Iteration 1351, loss = 0.08475381\n",
      "Iteration 1352, loss = 0.08520325\n",
      "Iteration 1353, loss = 0.08467672\n",
      "Iteration 1354, loss = 0.08477005\n",
      "Iteration 1355, loss = 0.08451653\n",
      "Iteration 1356, loss = 0.08456264\n",
      "Iteration 1357, loss = 0.08438777\n",
      "Iteration 1358, loss = 0.08443941\n",
      "Iteration 1359, loss = 0.08428070\n",
      "Iteration 1360, loss = 0.08415873\n",
      "Iteration 1361, loss = 0.08460513\n",
      "Iteration 1362, loss = 0.08415537\n",
      "Iteration 1363, loss = 0.08398795\n",
      "Iteration 1364, loss = 0.08401599\n",
      "Iteration 1365, loss = 0.08391335\n",
      "Iteration 1366, loss = 0.08395115\n",
      "Iteration 1367, loss = 0.08381219\n",
      "Iteration 1368, loss = 0.08385654\n",
      "Iteration 1369, loss = 0.08366682\n",
      "Iteration 1370, loss = 0.08360474\n",
      "Iteration 1371, loss = 0.08354987\n",
      "Iteration 1372, loss = 0.08368738\n",
      "Iteration 1373, loss = 0.08358711\n",
      "Iteration 1374, loss = 0.08332705\n",
      "Iteration 1375, loss = 0.08320414\n",
      "Iteration 1376, loss = 0.08315335\n",
      "Iteration 1377, loss = 0.08303854\n",
      "Iteration 1378, loss = 0.08321636\n",
      "Iteration 1379, loss = 0.08330512\n",
      "Iteration 1380, loss = 0.08302450\n",
      "Iteration 1381, loss = 0.08304721\n",
      "Iteration 1382, loss = 0.08278371\n",
      "Iteration 1383, loss = 0.08281323\n",
      "Iteration 1384, loss = 0.08311077\n",
      "Iteration 1385, loss = 0.08260820\n",
      "Iteration 1386, loss = 0.08248446\n",
      "Iteration 1387, loss = 0.08247005\n",
      "Iteration 1388, loss = 0.08246831\n",
      "Iteration 1389, loss = 0.08241135\n",
      "Iteration 1390, loss = 0.08230821\n",
      "Iteration 1391, loss = 0.08219951\n",
      "Iteration 1392, loss = 0.08211844\n",
      "Iteration 1393, loss = 0.08210953\n",
      "Iteration 1394, loss = 0.08201115\n",
      "Iteration 1395, loss = 0.08211311\n",
      "Iteration 1396, loss = 0.08195724\n",
      "Iteration 1397, loss = 0.08189213\n",
      "Iteration 1398, loss = 0.08193572\n",
      "Iteration 1399, loss = 0.08189352\n",
      "Iteration 1400, loss = 0.08163501\n",
      "Iteration 1401, loss = 0.08157865\n",
      "Iteration 1402, loss = 0.08156921\n",
      "Iteration 1403, loss = 0.08166049\n",
      "Iteration 1404, loss = 0.08145383\n",
      "Iteration 1405, loss = 0.08142087\n",
      "Iteration 1406, loss = 0.08137184\n",
      "Iteration 1407, loss = 0.08120898\n",
      "Iteration 1408, loss = 0.08165216\n",
      "Iteration 1409, loss = 0.08112033\n",
      "Iteration 1410, loss = 0.08122422\n",
      "Iteration 1411, loss = 0.08107268\n",
      "Iteration 1412, loss = 0.08088390\n",
      "Iteration 1413, loss = 0.08086532\n",
      "Iteration 1414, loss = 0.08085147\n",
      "Iteration 1415, loss = 0.08074988\n",
      "Iteration 1416, loss = 0.08088061\n",
      "Iteration 1417, loss = 0.08077037\n",
      "Iteration 1418, loss = 0.08071833\n",
      "Iteration 1419, loss = 0.08051023\n",
      "Iteration 1420, loss = 0.08052377\n",
      "Iteration 1421, loss = 0.08036440\n",
      "Iteration 1422, loss = 0.08042512\n",
      "Iteration 1423, loss = 0.08035318\n",
      "Iteration 1424, loss = 0.08023069\n",
      "Iteration 1425, loss = 0.08022858\n",
      "Iteration 1426, loss = 0.08020544\n",
      "Iteration 1427, loss = 0.08021529\n",
      "Iteration 1428, loss = 0.07996438\n",
      "Iteration 1429, loss = 0.07988857\n",
      "Iteration 1430, loss = 0.08104338\n",
      "Iteration 1431, loss = 0.08014744\n",
      "Iteration 1432, loss = 0.07997956\n",
      "Iteration 1433, loss = 0.07981875\n",
      "Iteration 1434, loss = 0.07966875\n",
      "Iteration 1435, loss = 0.07970006\n",
      "Iteration 1436, loss = 0.07953043\n",
      "Iteration 1437, loss = 0.07962975\n",
      "Iteration 1438, loss = 0.07951243\n",
      "Iteration 1439, loss = 0.07940268\n",
      "Iteration 1440, loss = 0.07930033\n",
      "Iteration 1441, loss = 0.07936287\n",
      "Iteration 1442, loss = 0.07924617\n",
      "Iteration 1443, loss = 0.07921631\n",
      "Iteration 1444, loss = 0.07911042\n",
      "Iteration 1445, loss = 0.07905793\n",
      "Iteration 1446, loss = 0.07903881\n",
      "Iteration 1447, loss = 0.07938771\n",
      "Iteration 1448, loss = 0.07904123\n",
      "Iteration 1449, loss = 0.07886036\n",
      "Iteration 1450, loss = 0.07879142\n",
      "Iteration 1451, loss = 0.07869992\n",
      "Iteration 1452, loss = 0.07868012\n",
      "Iteration 1453, loss = 0.07868277\n",
      "Iteration 1454, loss = 0.07856221\n",
      "Iteration 1455, loss = 0.07851471\n",
      "Iteration 1456, loss = 0.07851478\n",
      "Iteration 1457, loss = 0.07900871\n",
      "Iteration 1458, loss = 0.07830509\n",
      "Iteration 1459, loss = 0.07823795\n",
      "Iteration 1460, loss = 0.07821062\n",
      "Iteration 1461, loss = 0.07831453\n",
      "Iteration 1462, loss = 0.07839559\n",
      "Iteration 1463, loss = 0.07815671\n",
      "Iteration 1464, loss = 0.07824522\n",
      "Iteration 1465, loss = 0.07829921\n",
      "Iteration 1466, loss = 0.07792877\n",
      "Iteration 1467, loss = 0.07796703\n",
      "Iteration 1468, loss = 0.07797608\n",
      "Iteration 1469, loss = 0.07779447\n",
      "Iteration 1470, loss = 0.07774828\n",
      "Iteration 1471, loss = 0.07782207\n",
      "Iteration 1472, loss = 0.07773520\n",
      "Iteration 1473, loss = 0.07756977\n",
      "Iteration 1474, loss = 0.07755434\n",
      "Iteration 1475, loss = 0.07749282\n",
      "Iteration 1476, loss = 0.07741667\n",
      "Iteration 1477, loss = 0.07755968\n",
      "Iteration 1478, loss = 0.07748461\n",
      "Iteration 1479, loss = 0.07741212\n",
      "Iteration 1480, loss = 0.07733633\n",
      "Iteration 1481, loss = 0.07722338\n",
      "Iteration 1482, loss = 0.07738549\n",
      "Iteration 1483, loss = 0.07734343\n",
      "Iteration 1484, loss = 0.07715599\n",
      "Iteration 1485, loss = 0.07722007\n",
      "Iteration 1486, loss = 0.07711831\n",
      "Iteration 1487, loss = 0.07696061\n",
      "Iteration 1488, loss = 0.07686907\n",
      "Iteration 1489, loss = 0.07707235\n",
      "Iteration 1490, loss = 0.07684298\n",
      "Iteration 1491, loss = 0.07688362\n",
      "Iteration 1492, loss = 0.07663235\n",
      "Iteration 1493, loss = 0.07686851\n",
      "Iteration 1494, loss = 0.07644594\n",
      "Iteration 1495, loss = 0.07650214\n",
      "Iteration 1496, loss = 0.07650580\n",
      "Iteration 1497, loss = 0.07661592\n",
      "Iteration 1498, loss = 0.07640142\n",
      "Iteration 1499, loss = 0.07663404\n",
      "Iteration 1500, loss = 0.07646706\n",
      "Iteration 1501, loss = 0.07618963\n",
      "Iteration 1502, loss = 0.07624978\n",
      "Iteration 1503, loss = 0.07613152\n",
      "Iteration 1504, loss = 0.07626453\n",
      "Iteration 1505, loss = 0.07623476\n",
      "Iteration 1506, loss = 0.07596463\n",
      "Iteration 1507, loss = 0.07607954\n",
      "Iteration 1508, loss = 0.07600840\n",
      "Iteration 1509, loss = 0.07579149\n",
      "Iteration 1510, loss = 0.07579942\n",
      "Iteration 1511, loss = 0.07571756\n",
      "Iteration 1512, loss = 0.07571340\n",
      "Iteration 1513, loss = 0.07570619\n",
      "Iteration 1514, loss = 0.07557442\n",
      "Iteration 1515, loss = 0.07581400\n",
      "Iteration 1516, loss = 0.07544362\n",
      "Iteration 1517, loss = 0.07537880\n",
      "Iteration 1518, loss = 0.07554981\n",
      "Iteration 1519, loss = 0.07545392\n",
      "Iteration 1520, loss = 0.07534342\n",
      "Iteration 1521, loss = 0.07548088\n",
      "Iteration 1522, loss = 0.07512947\n",
      "Iteration 1523, loss = 0.07512902\n",
      "Iteration 1524, loss = 0.07545847\n",
      "Iteration 1525, loss = 0.07518463\n",
      "Iteration 1526, loss = 0.07541894\n",
      "Iteration 1527, loss = 0.07509446\n",
      "Iteration 1528, loss = 0.07496901\n",
      "Iteration 1529, loss = 0.07492952\n",
      "Iteration 1530, loss = 0.07519221\n",
      "Iteration 1531, loss = 0.07484737\n",
      "Iteration 1532, loss = 0.07482482\n",
      "Iteration 1533, loss = 0.07478840\n",
      "Iteration 1534, loss = 0.07502039\n",
      "Iteration 1535, loss = 0.07469325\n",
      "Iteration 1536, loss = 0.07470239\n",
      "Iteration 1537, loss = 0.07494326\n",
      "Iteration 1538, loss = 0.07454360\n",
      "Iteration 1539, loss = 0.07456862\n",
      "Iteration 1540, loss = 0.07445364\n",
      "Iteration 1541, loss = 0.07448270\n",
      "Iteration 1542, loss = 0.07432360\n",
      "Iteration 1543, loss = 0.07445501\n",
      "Iteration 1544, loss = 0.07425944\n",
      "Iteration 1545, loss = 0.07420844\n",
      "Iteration 1546, loss = 0.07417425\n",
      "Iteration 1547, loss = 0.07427046\n",
      "Iteration 1548, loss = 0.07416527\n",
      "Iteration 1549, loss = 0.07405081\n",
      "Iteration 1550, loss = 0.07400809\n",
      "Iteration 1551, loss = 0.07408487\n",
      "Iteration 1552, loss = 0.07392100\n",
      "Iteration 1553, loss = 0.07439446\n",
      "Iteration 1554, loss = 0.07379956\n",
      "Iteration 1555, loss = 0.07383829\n",
      "Iteration 1556, loss = 0.07375539\n",
      "Iteration 1557, loss = 0.07373316\n",
      "Iteration 1558, loss = 0.07382647\n",
      "Iteration 1559, loss = 0.07358556\n",
      "Iteration 1560, loss = 0.07373519\n",
      "Iteration 1561, loss = 0.07373130\n",
      "Iteration 1562, loss = 0.07347354\n",
      "Iteration 1563, loss = 0.07368588\n",
      "Iteration 1564, loss = 0.07343981\n",
      "Iteration 1565, loss = 0.07346905\n",
      "Iteration 1566, loss = 0.07339591\n",
      "Iteration 1567, loss = 0.07339256\n",
      "Iteration 1568, loss = 0.07331327\n",
      "Iteration 1569, loss = 0.07327237\n",
      "Iteration 1570, loss = 0.07332733\n",
      "Iteration 1571, loss = 0.07339743\n",
      "Iteration 1572, loss = 0.07317813\n",
      "Iteration 1573, loss = 0.07354520\n",
      "Iteration 1574, loss = 0.07310666\n",
      "Iteration 1575, loss = 0.07357554\n",
      "Iteration 1576, loss = 0.07336324\n",
      "Iteration 1577, loss = 0.07318224\n",
      "Iteration 1578, loss = 0.07286504\n",
      "Iteration 1579, loss = 0.07298190\n",
      "Iteration 1580, loss = 0.07285827\n",
      "Iteration 1581, loss = 0.07269411\n",
      "Iteration 1582, loss = 0.07266660\n",
      "Iteration 1583, loss = 0.07271828\n",
      "Iteration 1584, loss = 0.07268744\n",
      "Iteration 1585, loss = 0.07266809\n",
      "Iteration 1586, loss = 0.07275432\n",
      "Iteration 1587, loss = 0.07286178\n",
      "Iteration 1588, loss = 0.07254029\n",
      "Iteration 1589, loss = 0.07267992\n",
      "Iteration 1590, loss = 0.07268346\n",
      "Iteration 1591, loss = 0.07232444\n",
      "Iteration 1592, loss = 0.07229875\n",
      "Iteration 1593, loss = 0.07238322\n",
      "Iteration 1594, loss = 0.07236674\n",
      "Iteration 1595, loss = 0.07225819\n",
      "Iteration 1596, loss = 0.07262305\n",
      "Iteration 1597, loss = 0.07214003\n",
      "Iteration 1598, loss = 0.07203786\n",
      "Iteration 1599, loss = 0.07214296\n",
      "Iteration 1600, loss = 0.07199812\n",
      "Iteration 1601, loss = 0.07197165\n",
      "Iteration 1602, loss = 0.07230704\n",
      "Iteration 1603, loss = 0.07195371\n",
      "Iteration 1604, loss = 0.07202074\n",
      "Iteration 1605, loss = 0.07184445\n",
      "Iteration 1606, loss = 0.07177498\n",
      "Iteration 1607, loss = 0.07172052\n",
      "Iteration 1608, loss = 0.07187962\n",
      "Iteration 1609, loss = 0.07177830\n",
      "Iteration 1610, loss = 0.07159449\n",
      "Iteration 1611, loss = 0.07172173\n",
      "Iteration 1612, loss = 0.07152923\n",
      "Iteration 1613, loss = 0.07189157\n",
      "Iteration 1614, loss = 0.07144008\n",
      "Iteration 1615, loss = 0.07139753\n",
      "Iteration 1616, loss = 0.07216220\n",
      "Iteration 1617, loss = 0.07183983\n",
      "Iteration 1618, loss = 0.07140639\n",
      "Iteration 1619, loss = 0.07136162\n",
      "Iteration 1620, loss = 0.07126242\n",
      "Iteration 1621, loss = 0.07127220\n",
      "Iteration 1622, loss = 0.07117826\n",
      "Iteration 1623, loss = 0.07123025\n",
      "Iteration 1624, loss = 0.07126512\n",
      "Iteration 1625, loss = 0.07134122\n",
      "Iteration 1626, loss = 0.07117394\n",
      "Iteration 1627, loss = 0.07114616\n",
      "Iteration 1628, loss = 0.07105984\n",
      "Iteration 1629, loss = 0.07126991\n",
      "Iteration 1630, loss = 0.07089580\n",
      "Iteration 1631, loss = 0.07114048\n",
      "Iteration 1632, loss = 0.07082625\n",
      "Iteration 1633, loss = 0.07080245\n",
      "Iteration 1634, loss = 0.07085183\n",
      "Iteration 1635, loss = 0.07078364\n",
      "Iteration 1636, loss = 0.07090818\n",
      "Iteration 1637, loss = 0.07067007\n",
      "Iteration 1638, loss = 0.07061776\n",
      "Iteration 1639, loss = 0.07056446\n",
      "Iteration 1640, loss = 0.07073112\n",
      "Iteration 1641, loss = 0.07065859\n",
      "Iteration 1642, loss = 0.07047092\n",
      "Iteration 1643, loss = 0.07054808\n",
      "Iteration 1644, loss = 0.07040762\n",
      "Iteration 1645, loss = 0.07039759\n",
      "Iteration 1646, loss = 0.07031099\n",
      "Iteration 1647, loss = 0.07028615\n",
      "Iteration 1648, loss = 0.07026741\n",
      "Iteration 1649, loss = 0.07019198\n",
      "Iteration 1650, loss = 0.07038686\n",
      "Iteration 1651, loss = 0.07014249\n",
      "Iteration 1652, loss = 0.07008647\n",
      "Iteration 1653, loss = 0.07043256\n",
      "Iteration 1654, loss = 0.07014417\n",
      "Iteration 1655, loss = 0.07018609\n",
      "Iteration 1656, loss = 0.07012654\n",
      "Iteration 1657, loss = 0.07006281\n",
      "Iteration 1658, loss = 0.07030564\n",
      "Iteration 1659, loss = 0.06996462\n",
      "Iteration 1660, loss = 0.06983003\n",
      "Iteration 1661, loss = 0.06987100\n",
      "Iteration 1662, loss = 0.06981843\n",
      "Iteration 1663, loss = 0.06982069\n",
      "Iteration 1664, loss = 0.06996879\n",
      "Iteration 1665, loss = 0.06977949\n",
      "Iteration 1666, loss = 0.06972527\n",
      "Iteration 1667, loss = 0.06976641\n",
      "Iteration 1668, loss = 0.06988314\n",
      "Iteration 1669, loss = 0.06962451\n",
      "Iteration 1670, loss = 0.06951860\n",
      "Iteration 1671, loss = 0.06956034\n",
      "Iteration 1672, loss = 0.06997957\n",
      "Iteration 1673, loss = 0.07001619\n",
      "Iteration 1674, loss = 0.06956380\n",
      "Iteration 1675, loss = 0.06982209\n",
      "Iteration 1676, loss = 0.06944072\n",
      "Iteration 1677, loss = 0.06929345\n",
      "Iteration 1678, loss = 0.06930020\n",
      "Iteration 1679, loss = 0.06963642\n",
      "Iteration 1680, loss = 0.06919170\n",
      "Iteration 1681, loss = 0.06922255\n",
      "Iteration 1682, loss = 0.06928232\n",
      "Iteration 1683, loss = 0.06912727\n",
      "Iteration 1684, loss = 0.06920853\n",
      "Iteration 1685, loss = 0.06965713\n",
      "Iteration 1686, loss = 0.06901648\n",
      "Iteration 1687, loss = 0.06899588\n",
      "Iteration 1688, loss = 0.06915755\n",
      "Iteration 1689, loss = 0.06930387\n",
      "Iteration 1690, loss = 0.06950829\n",
      "Iteration 1691, loss = 0.06883314\n",
      "Iteration 1692, loss = 0.06882788\n",
      "Iteration 1693, loss = 0.06881013\n",
      "Iteration 1694, loss = 0.06891953\n",
      "Iteration 1695, loss = 0.06872704\n",
      "Iteration 1696, loss = 0.06872406\n",
      "Iteration 1697, loss = 0.06883716\n",
      "Iteration 1698, loss = 0.06869925\n",
      "Iteration 1699, loss = 0.06883335\n",
      "Iteration 1700, loss = 0.06853356\n",
      "Iteration 1701, loss = 0.06850246\n",
      "Iteration 1702, loss = 0.06854470\n",
      "Iteration 1703, loss = 0.06855902\n",
      "Iteration 1704, loss = 0.06858615\n",
      "Iteration 1705, loss = 0.06845791\n",
      "Iteration 1706, loss = 0.06844638\n",
      "Iteration 1707, loss = 0.06902862\n",
      "Iteration 1708, loss = 0.06834936\n",
      "Iteration 1709, loss = 0.06890408\n",
      "Iteration 1710, loss = 0.06839512\n",
      "Iteration 1711, loss = 0.06832507\n",
      "Iteration 1712, loss = 0.06862798\n",
      "Iteration 1713, loss = 0.06816210\n",
      "Iteration 1714, loss = 0.06814980\n",
      "Iteration 1715, loss = 0.06807911\n",
      "Iteration 1716, loss = 0.06820045\n",
      "Iteration 1717, loss = 0.06816198\n",
      "Iteration 1718, loss = 0.06808674\n",
      "Iteration 1719, loss = 0.06810250\n",
      "Iteration 1720, loss = 0.06845958\n",
      "Iteration 1721, loss = 0.06805972\n",
      "Iteration 1722, loss = 0.06809019\n",
      "Iteration 1723, loss = 0.06791351\n",
      "Iteration 1724, loss = 0.06790467\n",
      "Iteration 1725, loss = 0.06785534\n",
      "Iteration 1726, loss = 0.06791627\n",
      "Iteration 1727, loss = 0.06780420\n",
      "Iteration 1728, loss = 0.06784838\n",
      "Iteration 1729, loss = 0.06776401\n",
      "Iteration 1730, loss = 0.06776334\n",
      "Iteration 1731, loss = 0.06762647\n",
      "Iteration 1732, loss = 0.06772047\n",
      "Iteration 1733, loss = 0.06779154\n",
      "Iteration 1734, loss = 0.06780932\n",
      "Iteration 1735, loss = 0.06757242\n",
      "Iteration 1736, loss = 0.06754206\n",
      "Iteration 1737, loss = 0.06748017\n",
      "Iteration 1738, loss = 0.06752257\n",
      "Iteration 1739, loss = 0.06760123\n",
      "Iteration 1740, loss = 0.06748250\n",
      "Iteration 1741, loss = 0.06787325\n",
      "Iteration 1742, loss = 0.06742106\n",
      "Iteration 1743, loss = 0.06738240\n",
      "Iteration 1744, loss = 0.06789880\n",
      "Iteration 1745, loss = 0.06747754\n",
      "Iteration 1746, loss = 0.06726983\n",
      "Iteration 1747, loss = 0.06726520\n",
      "Iteration 1748, loss = 0.06742035\n",
      "Iteration 1749, loss = 0.06719487\n",
      "Iteration 1750, loss = 0.06714411\n",
      "Iteration 1751, loss = 0.06708303\n",
      "Iteration 1752, loss = 0.06720914\n",
      "Iteration 1753, loss = 0.06716566\n",
      "Iteration 1754, loss = 0.06788199\n",
      "Iteration 1755, loss = 0.06698663\n",
      "Iteration 1756, loss = 0.06706984\n",
      "Iteration 1757, loss = 0.06705939\n",
      "Iteration 1758, loss = 0.06685331\n",
      "Iteration 1759, loss = 0.06689506\n",
      "Iteration 1760, loss = 0.06693913\n",
      "Iteration 1761, loss = 0.06696726\n",
      "Iteration 1762, loss = 0.06718733\n",
      "Iteration 1763, loss = 0.06709184\n",
      "Iteration 1764, loss = 0.06714785\n",
      "Iteration 1765, loss = 0.06672416\n",
      "Iteration 1766, loss = 0.06675663\n",
      "Iteration 1767, loss = 0.06717597\n",
      "Iteration 1768, loss = 0.06662544\n",
      "Iteration 1769, loss = 0.06682539\n",
      "Iteration 1770, loss = 0.06663946\n",
      "Iteration 1771, loss = 0.06659685\n",
      "Iteration 1772, loss = 0.06696397\n",
      "Iteration 1773, loss = 0.06655651\n",
      "Iteration 1774, loss = 0.06643884\n",
      "Iteration 1775, loss = 0.06645579\n",
      "Iteration 1776, loss = 0.06694246\n",
      "Iteration 1777, loss = 0.06657915\n",
      "Iteration 1778, loss = 0.06640006\n",
      "Iteration 1779, loss = 0.06651413\n",
      "Iteration 1780, loss = 0.06641671\n",
      "Iteration 1781, loss = 0.06642561\n",
      "Iteration 1782, loss = 0.06642525\n",
      "Iteration 1783, loss = 0.06626145\n",
      "Iteration 1784, loss = 0.06622432\n",
      "Iteration 1785, loss = 0.06630815\n",
      "Iteration 1786, loss = 0.06658043\n",
      "Iteration 1787, loss = 0.06622883\n",
      "Iteration 1788, loss = 0.06617675\n",
      "Iteration 1789, loss = 0.06611812\n",
      "Iteration 1790, loss = 0.06618025\n",
      "Iteration 1791, loss = 0.06618429\n",
      "Iteration 1792, loss = 0.06637040\n",
      "Iteration 1793, loss = 0.06626661\n",
      "Iteration 1794, loss = 0.06612163\n",
      "Iteration 1795, loss = 0.06596891\n",
      "Iteration 1796, loss = 0.06629851\n",
      "Iteration 1797, loss = 0.06612059\n",
      "Iteration 1798, loss = 0.06595267\n",
      "Iteration 1799, loss = 0.06596055\n",
      "Iteration 1800, loss = 0.06604238\n",
      "Iteration 1801, loss = 0.06583097\n",
      "Iteration 1802, loss = 0.06580975\n",
      "Iteration 1803, loss = 0.06580970\n",
      "Iteration 1804, loss = 0.06576750\n",
      "Iteration 1805, loss = 0.06577666\n",
      "Iteration 1806, loss = 0.06569177\n",
      "Iteration 1807, loss = 0.06590442\n",
      "Iteration 1808, loss = 0.06562732\n",
      "Iteration 1809, loss = 0.06565900\n",
      "Iteration 1810, loss = 0.06571688\n",
      "Iteration 1811, loss = 0.06563729\n",
      "Iteration 1812, loss = 0.06561102\n",
      "Iteration 1813, loss = 0.06559515\n",
      "Iteration 1814, loss = 0.06574738\n",
      "Iteration 1815, loss = 0.06564105\n",
      "Iteration 1816, loss = 0.06548507\n",
      "Iteration 1817, loss = 0.06563128\n",
      "Iteration 1818, loss = 0.06547259\n",
      "Iteration 1819, loss = 0.06550815\n",
      "Iteration 1820, loss = 0.06540820\n",
      "Iteration 1821, loss = 0.06532753\n",
      "Iteration 1822, loss = 0.06548323\n",
      "Iteration 1823, loss = 0.06535954\n",
      "Iteration 1824, loss = 0.06532430\n",
      "Iteration 1825, loss = 0.06526893\n",
      "Iteration 1826, loss = 0.06539189\n",
      "Iteration 1827, loss = 0.06518690\n",
      "Iteration 1828, loss = 0.06539913\n",
      "Iteration 1829, loss = 0.06512900\n",
      "Iteration 1830, loss = 0.06515344\n",
      "Iteration 1831, loss = 0.06519764\n",
      "Iteration 1832, loss = 0.06548929\n",
      "Iteration 1833, loss = 0.06529667\n",
      "Iteration 1834, loss = 0.06539144\n",
      "Iteration 1835, loss = 0.06505966\n",
      "Iteration 1836, loss = 0.06516896\n",
      "Iteration 1837, loss = 0.06496815\n",
      "Iteration 1838, loss = 0.06493797\n",
      "Iteration 1839, loss = 0.06491534\n",
      "Iteration 1840, loss = 0.06486951\n",
      "Iteration 1841, loss = 0.06489258\n",
      "Iteration 1842, loss = 0.06488247\n",
      "Iteration 1843, loss = 0.06499150\n",
      "Iteration 1844, loss = 0.06497900\n",
      "Iteration 1845, loss = 0.06476413\n",
      "Iteration 1846, loss = 0.06491253\n",
      "Iteration 1847, loss = 0.06481865\n",
      "Iteration 1848, loss = 0.06475381\n",
      "Iteration 1849, loss = 0.06469058\n",
      "Iteration 1850, loss = 0.06482030\n",
      "Iteration 1851, loss = 0.06462632\n",
      "Iteration 1852, loss = 0.06466350\n",
      "Iteration 1853, loss = 0.06469930\n",
      "Iteration 1854, loss = 0.06457143\n",
      "Iteration 1855, loss = 0.06461089\n",
      "Iteration 1856, loss = 0.06457871\n",
      "Iteration 1857, loss = 0.06452427\n",
      "Iteration 1858, loss = 0.06488560\n",
      "Iteration 1859, loss = 0.06475138\n",
      "Iteration 1860, loss = 0.06454771\n",
      "Iteration 1861, loss = 0.06439613\n",
      "Iteration 1862, loss = 0.06462226\n",
      "Iteration 1863, loss = 0.06465394\n",
      "Iteration 1864, loss = 0.06458992\n",
      "Iteration 1865, loss = 0.06439262\n",
      "Iteration 1866, loss = 0.06435230\n",
      "Iteration 1867, loss = 0.06431184\n",
      "Iteration 1868, loss = 0.06426748\n",
      "Iteration 1869, loss = 0.06442050\n",
      "Iteration 1870, loss = 0.06429811\n",
      "Iteration 1871, loss = 0.06425214\n",
      "Iteration 1872, loss = 0.06430067\n",
      "Iteration 1873, loss = 0.06416727\n",
      "Iteration 1874, loss = 0.06415958\n",
      "Iteration 1875, loss = 0.06414998\n",
      "Iteration 1876, loss = 0.06444032\n",
      "Iteration 1877, loss = 0.06431526\n",
      "Iteration 1878, loss = 0.06411358\n",
      "Iteration 1879, loss = 0.06431335\n",
      "Iteration 1880, loss = 0.06408212\n",
      "Iteration 1881, loss = 0.06432670\n",
      "Iteration 1882, loss = 0.06391923\n",
      "Iteration 1883, loss = 0.06405601\n",
      "Iteration 1884, loss = 0.06398346\n",
      "Iteration 1885, loss = 0.06394669\n",
      "Iteration 1886, loss = 0.06395824\n",
      "Iteration 1887, loss = 0.06386105\n",
      "Iteration 1888, loss = 0.06383126\n",
      "Iteration 1889, loss = 0.06397438\n",
      "Iteration 1890, loss = 0.06386607\n",
      "Iteration 1891, loss = 0.06384349\n",
      "Iteration 1892, loss = 0.06379896\n",
      "Iteration 1893, loss = 0.06385522\n",
      "Iteration 1894, loss = 0.06380983\n",
      "Iteration 1895, loss = 0.06374563\n",
      "Iteration 1896, loss = 0.06379696\n",
      "Iteration 1897, loss = 0.06385273\n",
      "Iteration 1898, loss = 0.06363709\n",
      "Iteration 1899, loss = 0.06364553\n",
      "Iteration 1900, loss = 0.06359072\n",
      "Iteration 1901, loss = 0.06358887\n",
      "Iteration 1902, loss = 0.06358737\n",
      "Iteration 1903, loss = 0.06400564\n",
      "Iteration 1904, loss = 0.06348442\n",
      "Iteration 1905, loss = 0.06355904\n",
      "Iteration 1906, loss = 0.06366636\n",
      "Iteration 1907, loss = 0.06399822\n",
      "Iteration 1908, loss = 0.06350089\n",
      "Iteration 1909, loss = 0.06348543\n",
      "Iteration 1910, loss = 0.06353604\n",
      "Iteration 1911, loss = 0.06341288\n",
      "Iteration 1912, loss = 0.06332022\n",
      "Iteration 1913, loss = 0.06345497\n",
      "Iteration 1914, loss = 0.06352231\n",
      "Iteration 1915, loss = 0.06345468\n",
      "Iteration 1916, loss = 0.06340524\n",
      "Iteration 1917, loss = 0.06338700\n",
      "Iteration 1918, loss = 0.06331370\n",
      "Iteration 1919, loss = 0.06337645\n",
      "Iteration 1920, loss = 0.06326020\n",
      "Iteration 1921, loss = 0.06329282\n",
      "Iteration 1922, loss = 0.06319340\n",
      "Iteration 1923, loss = 0.06334721\n",
      "Iteration 1924, loss = 0.06340929\n",
      "Iteration 1925, loss = 0.06338685\n",
      "Iteration 1926, loss = 0.06306841\n",
      "Iteration 1927, loss = 0.06322972\n",
      "Iteration 1928, loss = 0.06302962\n",
      "Iteration 1929, loss = 0.06306736\n",
      "Iteration 1930, loss = 0.06305531\n",
      "Iteration 1931, loss = 0.06315799\n",
      "Iteration 1932, loss = 0.06298221\n",
      "Iteration 1933, loss = 0.06305503\n",
      "Iteration 1934, loss = 0.06313951\n",
      "Iteration 1935, loss = 0.06317196\n",
      "Iteration 1936, loss = 0.06305991\n",
      "Iteration 1937, loss = 0.06318129\n",
      "Iteration 1938, loss = 0.06289820\n",
      "Iteration 1939, loss = 0.06287956\n",
      "Iteration 1940, loss = 0.06283497\n",
      "Iteration 1941, loss = 0.06294606\n",
      "Iteration 1942, loss = 0.06287756\n",
      "Iteration 1943, loss = 0.06288437\n",
      "Iteration 1944, loss = 0.06288994\n",
      "Iteration 1945, loss = 0.06296206\n",
      "Iteration 1946, loss = 0.06277667\n",
      "Iteration 1947, loss = 0.06277952\n",
      "Iteration 1948, loss = 0.06272397\n",
      "Iteration 1949, loss = 0.06271957\n",
      "Iteration 1950, loss = 0.06285650\n",
      "Iteration 1951, loss = 0.06265686\n",
      "Iteration 1952, loss = 0.06262924\n",
      "Iteration 1953, loss = 0.06266195\n",
      "Iteration 1954, loss = 0.06259336\n",
      "Iteration 1955, loss = 0.06260308\n",
      "Iteration 1956, loss = 0.06294598\n",
      "Iteration 1957, loss = 0.06253716\n",
      "Iteration 1958, loss = 0.06275802\n",
      "Iteration 1959, loss = 0.06249305\n",
      "Iteration 1960, loss = 0.06246623\n",
      "Iteration 1961, loss = 0.06268771\n",
      "Iteration 1962, loss = 0.06242593\n",
      "Iteration 1963, loss = 0.06252967\n",
      "Iteration 1964, loss = 0.06266499\n",
      "Iteration 1965, loss = 0.06256886\n",
      "Iteration 1966, loss = 0.06267959\n",
      "Iteration 1967, loss = 0.06257045\n",
      "Iteration 1968, loss = 0.06230717\n",
      "Iteration 1969, loss = 0.06232776\n",
      "Iteration 1970, loss = 0.06242290\n",
      "Iteration 1971, loss = 0.06244847\n",
      "Iteration 1972, loss = 0.06257270\n",
      "Iteration 1973, loss = 0.06247265\n",
      "Iteration 1974, loss = 0.06298267\n",
      "Iteration 1975, loss = 0.06221652\n",
      "Iteration 1976, loss = 0.06217317\n",
      "Iteration 1977, loss = 0.06211919\n",
      "Iteration 1978, loss = 0.06221213\n",
      "Iteration 1979, loss = 0.06218331\n",
      "Iteration 1980, loss = 0.06209353\n",
      "Iteration 1981, loss = 0.06211881\n",
      "Iteration 1982, loss = 0.06222573\n",
      "Iteration 1983, loss = 0.06233025\n",
      "Iteration 1984, loss = 0.06207498\n",
      "Iteration 1985, loss = 0.06208701\n",
      "Iteration 1986, loss = 0.06200965\n",
      "Iteration 1987, loss = 0.06212198\n",
      "Iteration 1988, loss = 0.06193774\n",
      "Iteration 1989, loss = 0.06212742\n",
      "Iteration 1990, loss = 0.06231609\n",
      "Iteration 1991, loss = 0.06200027\n",
      "Iteration 1992, loss = 0.06203539\n",
      "Iteration 1993, loss = 0.06191117\n",
      "Iteration 1994, loss = 0.06189232\n",
      "Iteration 1995, loss = 0.06197614\n",
      "Iteration 1996, loss = 0.06187607\n",
      "Iteration 1997, loss = 0.06181804\n",
      "Iteration 1998, loss = 0.06204147\n",
      "Iteration 1999, loss = 0.06175636\n",
      "Iteration 2000, loss = 0.06196966\n",
      "Model accuracy: 96.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vishulaw\\Workspace\\genai-sandbox\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "network = MLPClassifier(max_iter=2000, \n",
    "                        verbose=True,\n",
    "                        tol=0.0000100,\n",
    "                        activation='logistic',\n",
    "                        solver='adam',\n",
    "                        learning_rate='constant',\n",
    "                        learning_rate_init=0.001, # Initial learning rate\n",
    "                        batch_size=32, \n",
    "                        hidden_layer_sizes=(4, 4)\n",
    "                        # early_stopping=True,\n",
    "                        # n_iter_no_change=50     # Two hidden layers with 4 neurons each\n",
    "                        )\n",
    "# Train the model\n",
    "network.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "accuracy = network.score(X_test, y_test)\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee27279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "network.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "837983c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.61728554, -0.68735379,  1.44936545,  1.39968041]),\n",
       " array([-1.36168079, -1.14100533,  2.2084117 ,  2.60953858]),\n",
       " array([ 1.06523705, -1.39278698,  0.55548802])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "network.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7054c4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.out_activation_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9559c",
   "metadata": {},
   "source": [
    "## Neural Network Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0004c26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21729bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 2, 0, 2, 1, 0, 0, 1, 0, 2, 1, 2, 1, 1, 2, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = network.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c322e228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 2, 0, 2, 1, 0, 0, 1, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dccf203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.67%\n",
      "Confusion Matrix:\n",
      "[[14  0  0]\n",
      " [ 0 10  1]\n",
      " [ 0  0  5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9605615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yellowbrick in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (1.5)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from yellowbrick) (3.10.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from yellowbrick) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from yellowbrick) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from yellowbrick) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from yellowbrick) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vishulaw\\workspace\\genai-sandbox\\venv\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (3.6.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'MLPClassifier Confusion Matrix'}, xlabel='Predicted Class', ylabel='True Class'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install yellowbrick --upgrade\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(network, classes=iris.target_names)\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)\n",
    "cm.show()  # Display the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a9b29",
   "metadata": {},
   "source": [
    "# Nueral Network Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9951744f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.3, 3.3, 4.7, 1.6]), np.int64(1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], y_test[0], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e5b352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aa1c752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = X_test[0].reshape(1, -1)\n",
    "network.predict(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb47b6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('versicolor')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names[network.predict(new)[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
